
ä¸€ã€ æµ‹è·æ³•
ä¼ ç»Ÿå¯¹ç„¦æŠ€æœ¯ä½¿ç”¨æ¯”è¾ƒå¹¿æ³›çš„å°±æ˜¯æµ‹è·æ³•ï¼Œå…¶åŸºæœ¬åŸç†å°±æ˜¯é€šè¿‡çº¢å¤–ã€æ¿€å…‰ä»¥åŠè¶…å£°æ³¢ç­‰è®¾å¤‡æµ‹é‡ç›®æ ‡ç‰©ä½“ä¸é€é•œä¹‹é—´çš„è·ç¦»ï¼Œå³ç‰©è·ï¼Œå†æ ¹æ®é«˜æ–¯æˆåƒå…¬å¼ è®¡ç®—å¯¹åº”çš„åƒè·ï¼Œæœ€åè°ƒæ•´ç‰©è·ï¼Œä»¥æ­¤æ¥è¿›è¡Œç³»ç»Ÿçš„è‡ªåŠ¨å¯¹ç„¦ã€‚å¦‚ä¸‹å…¬å¼ä¸ºç†æƒ³çš„å…‰å­¦æˆåƒç³»ç»Ÿæ¨¡å‹ã€‚è®¾é€é•œçš„ç„¦è·ä¸ºfï¼Œå­”å¾„ä¸ºDã€‚ç›®æ ‡ç‰©ä½“è¦é€šè¿‡é€é•œèƒ½å¤Ÿæ¸…æ™°æˆåƒï¼Œå¿…é¡»è¦æ»¡è¶³é«˜æ–¯æˆåƒå…¬å¼ï¼Œè¡¨ç¤ºä¸ºï¼š


å…¶ä¸­ï¼Œ Uè¡¨ç¤ºç‰©è·ï¼ŒVè¡¨ç¤ºåƒè·ã€‚æ ¹æ®é«˜æ–¯æˆåƒå…¬å¼å¯ä»¥çŸ¥é“ï¼Œå½“ç‰©è·Uå’Œé€é•œç„¦è·fçš„å€¼ç¡®å®šæ—¶ï¼Œç›®æ ‡ç‰©ä½“ä¼šå‘ˆç°æ¸…æ™°çš„æˆåƒç”»é¢ã€‚

ç”±äºå¯¹ç„¦è¿‡ç¨‹éœ€è¦æ‘„åƒå¤´ä¸»åŠ¨å‘å°„ä¿¡å·ï¼Œå†æ¥æ”¶ç›®æ ‡ç‰©ä½“åå°„å›æ¥çš„ä¿¡å·ï¼Œæ‰€ä»¥è¯¥æ–¹å¼å­˜åœ¨ä¸€ä¸ªç¼ºç‚¹ï¼Œå‡å¦‚è¿™ä¸ªä¿¡å·è¢«ç›®æ ‡ç‰©ä½“å¸æ”¶æˆ–è€…æ•£å°„ï¼Œé‚£ä¹ˆæ¥æ”¶ç«¯æ”¶åˆ°çš„ä¿¡å·æ¯”è¾ƒå¾®å¼±ç”šè‡³æ¥æ”¶ä¸åˆ°ä¿¡å·ï¼Œè¿™æ ·å°±æ— æ³•å‡†ç¡®å¾—åˆ°ç‰©è·ï¼Œå°†å¯¼è‡´å¯¹ç„¦å¤±è´¥ã€‚åŒæ—¶ï¼Œè¯¥æ–¹æ³•æ‰€éœ€è¦çš„è¾…åŠ©ä¿¡å·å‘é€æ¥æ”¶è£…ç½®ä¼šä½¿æ•´ä¸ªæˆåƒç³»ç»Ÿçš„ç¡¬ä»¶ä½“ç§¯å¢åŠ ï¼Œèƒ½è€—å˜é«˜ï¼Œä¸”ç³»ç»Ÿå¤æ‚åº¦é«˜ï¼Œæ˜“å—å¤–ç•Œç¯å¢ƒå¹²æ‰°ã€‚

äºŒã€ ç„¦ç‚¹æ£€æµ‹æ³•
ç„¦ç‚¹æ£€æµ‹æ³•ä¸»è¦åˆ†ä¸¤å¤§ç±»ï¼šå¯¹æ¯”åº¦æ£€æµ‹æ³•å’Œç›¸ä½å·®æ£€æµ‹æ³•ã€‚

1. å¯¹æ¯”åº¦æ£€æµ‹æ³•

å¯¹æ¯”åº¦æ˜¯æŒ‡ä¸€å¹…å›¾åƒå„åƒç´ ç‚¹ç°åº¦å€¼ä» 0 åˆ° 255 ä¹‹é—´å·®å¼‚çš„å¤§å°ã€‚å¯¹ç„¦æˆåŠŸçš„å›¾åƒçš„å¯¹æ¯”åº¦æœ€å¤§ï¼Œå›¾åƒä¸­ç‰©ä½“è¾¹ç¼˜è½®å»“è¶Šæ¸…æ™°ï¼Œè‰²å½©è¶Šç”ŸåŠ¨æ˜è‰³ã€‚


åŸºäºè¯¥åŸç†ï¼Œåœ¨è·ç¦»ç„¦å¹³é¢ç›¸ç­‰çš„ä¸¤ä¸ªä½ç½®ï¼Œåˆ†åˆ«æ”¾ç½®ä¸€ä¸ªä¼ æ„Ÿå™¨ï¼Œè¯¥ä¼ æ„Ÿå™¨å¯ä»¥æ”¶é›†å›¾åƒçš„å¯¹æ¯”åº¦ä¿¡æ¯ã€‚ç„¶åæ§åˆ¶ç”µæœºç§»åŠ¨ç„¦å¹³é¢ï¼ŒC1å’ŒC2å¾—åˆ°å›¾åƒçš„å¯¹æ¯”åº¦å˜åŒ–æ›²çº¿ã€‚ä¸¤æ¡æ›²çº¿ç›¸äº¤çš„ä½ç½®ï¼Œå³ä¸ºæ­£ç„¦ä½ç½®ã€‚


å›¾1 å¯¹æ¯”åº¦æ£€æµ‹æ³•
å¯¹æ¯”åº¦æ£€æµ‹æ³•ä¸»è¦åŸºäºæ„Ÿå…‰å™¨ä»¶åŠå›¾åƒå¤„ç†å™¨ï¼Œä¸éœ€è¦çš„ä¸€ä¸ªå•ç‹¬çš„å¯¹ç„¦è¾…åŠ©è£…ç½®ï¼Œä¸”ç²¾åº¦ä¹Ÿæ¯”æµ‹è·æ³•æ¥çš„æ›´é«˜ã€‚ä½†å› ä¸ºéœ€è¦å¤šæ¬¡é‡‡é›†å¯¹æ¯”åº¦ä¿¡æ¯ï¼Œå¹¶å¤šæ¬¡è®¡ç®—ï¼Œæ‰€ä»¥å¯¹ç„¦é€Ÿåº¦è¾ƒæ…¢ã€‚å¦å¤–ï¼Œè¿˜éœ€è¦ç›®æ ‡ç‰©ä½“å…·å¤‡è¶³å¤Ÿçš„å¯¹æ¯”åº¦ï¼Œå¯¹äºé‚£äº›çº¯è‰²æˆ–è€…åå·®ä¸æ˜æ˜¾çš„æƒ…å†µï¼Œè¯¥æ–¹æ³•å­˜åœ¨å¤±è´¥çš„å¯èƒ½ã€‚

2. ç›¸ä½æ£€æµ‹æ³•

ç›¸ä½æ£€æµ‹æ³•æ˜¯ç›®å‰å•åç›¸æœºæ™®éä½¿ç”¨çš„æ–¹æ³•ï¼Œä¸»è¦ä¾èµ–æ£€æµ‹ç›¸ä½å·®çš„è‡ªåŠ¨å¯¹ç„¦ä¼ æ„Ÿå™¨ã€‚å¦‚ä¸‹å›¾ï¼Œå…‰çº¿è¢«2ä¸ªæˆ–å¤šä¸ªå¸¦æœ‰å¤šä¸ªå¾®å°é€é•œçš„å°å‹å›¾åƒä¼ æ„Ÿå™¨æ¥æ”¶ã€‚äº‹å®ä¸Šï¼Œç°ä»£ç›¸ä½æ£€æµ‹è®¾å¤‡ä¸Šçš„ä¼ æ„Ÿå™¨æ•°é‡è¿œä¸æ­¢ä¸¤ä¸ª,è€Œä¸”è¿™äº›ä¼ æ„Ÿå™¨å½¼æ­¤éå¸¸é è¿‘ã€‚

å½“å…‰çº¿åˆ°è¾¾è¿™ä¸¤ä¸ªä¼ æ„Ÿå™¨æ—¶ï¼Œå¦‚æœç‰©ä½“å¤„äºå¯¹ç„¦çŠ¶æ€,åˆ™æ¥è‡ªé•œå¤´æœ€ä¸¤ä¾§çš„å…‰çº¿ä¼šèšåœ¨æ¯ä¸ªä¼ æ„Ÿå™¨çš„ä¸­å¿ƒ ã€‚ä¸¤ä¸ªä¼ æ„Ÿå™¨ä¸Šéƒ½ä¼šæœ‰ç›¸åŒçš„å›¾åƒ,è¡¨æ˜è¯¥ç‰©ä½“ç¡®å®å¤„äºåˆç„¦çŠ¶æ€ã€‚


å›¾2 ç›¸ä½æ£€æµ‹æ³•
ä¸Šå›¾çš„1è‡³4 è¡¨ç¤ºé•œå¤´èšç„¦çš„æƒ…å†µï¼Œé»„è‰²ä¸ºå›¾åƒä¼ æ„Ÿå™¨ï¼Œå¯ä»¥çœ‹åˆ°éšç€å›¾åƒä¼ æ„Ÿå™¨å’Œé•œå¤´çš„ä½ç½®å‘ç”Ÿç›¸å¯¹è¿åŠ¨ï¼Œä¸¤ä¸¤ä¸ªä¼ æ„Ÿå™¨çš„æ›²çº¿ä¹‹é—´äº§ç”Ÿç›¸ä½å·®ã€‚æœ‰ä¸€ç‚¹éœ€è¦æ³¨æ„ï¼Œåœ¨å®é™…çš„ç³»ç»Ÿä¸­ï¼Œæœ€ç»ˆç§»åŠ¨çš„æ˜¯é•œå¤´è€Œä¸æ˜¯ä¼ æ„Ÿå™¨ã€‚å¯¹ç„¦ç³»ç»Ÿé€šè¿‡æ£€æµ‹ä¿¡å·æ³¢å³°çš„ä½ç½®ï¼Œå°±å¯ä»¥çŸ¥é“åç§»çš„æ–¹å‘ï¼Œè®¡ç®—å‡ºç›¸ä½å·®ï¼Œå¾—å‡ºç›®æ ‡çš„è·ç¦»ï¼Œ æ§åˆ¶é•œå¤´å¾€å‰æˆ–è€…å¾€åç§»åŠ¨ï¼Œ è‡ªåŠ¨å®Œæˆå¯¹ç„¦ã€‚ è¯¥æ–¹æ³•å¯¹ç„¦é€Ÿåº¦å¿«ï¼Œåªéœ€è¦è®¡ç®—ä¸€æ¬¡å°±èƒ½å®Œæˆå¯¹ç„¦ã€‚

è¿‘å‡ å¹´,è¯¥æ–¹æ³•ä¹Ÿåœ¨æ‰‹æœºç›¸æœºä¸Šæµè¡Œèµ·æ¥,ä½†æ˜¯ç”±äºæ‰‹æœºç›¸æœºæ¨¡ç»„é«˜åº¦é›†æˆ,ä¸é€‚åˆå¢åŠ å•ç‹¬çš„å¯¹ç„¦ä¼ æ„Ÿå™¨,äºæ˜¯äººä»¬ä¾¿ç›´æ¥åœ¨æ‰‹æœºç›¸æœºçš„æ„Ÿå…‰å…ƒä»¶ä¸Šé¢„ç•™å‡ºæˆå¯¹çš„é®è”½åƒç´ ç‚¹æ¥è¿›è¡Œç›¸ä½æ£€æµ‹ã€‚

ä¸å¯¹æ¯”åº¦æ£€æµ‹æ³•ç›¸æ¯”è¾ƒï¼Œ ç›¸ä½å·®æ£€æµ‹æ³•[12]å¯¹ç„¦è¿‡ç¨‹ä¸€æ­¥åˆ°ä½ï¼Œä¸éœ€è¦é•œå¤´åå¤è¿åŠ¨ï¼Œä½†æ˜¯ç”±äºéœ€è¦ CCD æ„Ÿå…‰å…ƒä»¶ä¸Šé¢„ç•™çš„é®è”½åƒç´ ç‚¹æ¥è¿›è¡Œç›¸ä½æ£€æµ‹ï¼Œæ•…è€Œå¯¹ç¯å¢ƒå…‰çº¿è¦æ±‚æ¯”è¾ƒé«˜ï¼Œè¾ƒæš—çš„ç¯å¢ƒä¸‹ï¼Œå…¶å¯¹ç„¦æ•ˆæœä¸ç†æƒ³ã€‚

3. æ··åˆå¯¹ç„¦æ¨¡å¼

ç»è¿‡å¯¹ç„¦ç‚¹æ£€æµ‹æ³•ä¸¤å¤§ç±»æ–¹æ³•çš„åˆ†æï¼Œ æˆ‘ä»¬å‘ç°ä¸¤è€…ä¼˜åŠ¿åŠ£åŠ¿éƒ½è¾ƒä¸ºæ˜æ˜¾ï¼Œ ç°åœ¨äººä»¬é€šå¸¸æ˜¯å°†ä¸¤è€…ç›¸ç»“åˆï¼Œç§°ä¹‹ä¸ºæ··åˆå¯¹ç„¦æ¨¡å¼ï¼Œå……åˆ†å‘æŒ¥ä¸¤è€…çš„ä¼˜åŠ¿ï¼Œ æ— è®ºå¯¹ç„¦ç²¾åº¦ï¼Œè¿˜æ˜¯å¯¹ç„¦é€Ÿåº¦ï¼Œéƒ½å¾—åˆ°æ˜æ˜¾æ”¹å–„ï¼Œå¹¿æ³›åº”ç”¨äºå•åç›¸æœºå’Œæ‰‹æœºç›¸æœºé¢†åŸŸã€‚

ä¸‰ã€æ•°å­—å›¾åƒå¯¹ç„¦æ³•
åŸºäºæ•°å­—å›¾åƒçš„å¯¹ç„¦æŠ€æœ¯ï¼Œä¸»è¦å¯ä»¥åˆ†ä¸ºä¸¤å¤§ç±»ï¼šç¦»ç„¦æ·±åº¦æ³• DFDï¼ˆDepth From Defocusï¼‰ å’Œå¯¹ç„¦æ·±åº¦æ³• DFFï¼ˆDepth From Focusï¼‰ã€‚

1. ç¦»ç„¦æ·±åº¦æ³• DFD

ç¦»ç„¦æ·±åº¦æ³•DFDæ˜¯ä¸€ç§é€šè¿‡å»ºç«‹æ•°å­¦æ¨¡å‹çš„æ–¹æ³•æ¥å®ç°å¯¹ç„¦çš„æ–¹æ³•ï¼Œå…¶åŸºæœ¬æ€è·¯æ˜¯é€šè¿‡å¤§é‡é‡‡é›†ç¦»ç„¦çš„å›¾åƒï¼Œåˆ†æå›¾åƒå‚æ•°ï¼Œç¦»ç„¦ç¨‹åº¦å’Œæ¸…æ™°åº¦çš„å¯¹åº”å…³ç³»ï¼Œä»è€Œå»ºç«‹å¯¹ç„¦ç³»ç»Ÿçš„æ•°å­¦æ¨¡å‹ã€‚åˆ©ç”¨æ¨¡å‹ä¸­å¯¹åº”çš„æ¨¡ç³Šå›¾åƒæ¨å¯¼å‡ºä¸ç„¦å¹³é¢çš„åç§»é‡ï¼Œç„¶åé©±åŠ¨ä½ç§»å¹³å°ç§»åŠ¨åˆ°ç„¦å¹³é¢ï¼Œå®Œæˆæ•´ä¸ªå¯¹ç„¦è¿‡ç¨‹ã€‚è¿™ç§æ–¹æ³•å¯¹å»ºç«‹çš„æ•°å­¦æ¨¡å‹ç²¾åº¦è¦æ±‚å¾ˆé«˜ï¼Œé€šå¸¸åªéœ€è¦é‡‡é›†2åˆ°3å¹…å›¾åƒå³å¯å®Œæˆå¯¹ç„¦çš„è¿‡ç¨‹,ä½¿å¾—å¯¹ç„¦çš„æ—¶é—´å¤§å¤§ç¼©çŸ­ã€‚ç”±äºè¿™ç§æ–¹æ³•çš„å…³é”®åœ¨äºå¯¹ç„¦å‰ç³»ç»Ÿå·²ç»è·å–å¾…å¯¹ç„¦ç‰©ä½“çš„å¤§é‡å›¾åƒä¿¡æ¯ï¼Œä¸€æ—¦å¾…æµ‹ç¯å¢ƒæ”¹å˜ï¼Œéœ€è¦é‡æ–°æ‹Ÿåˆç¦»ç„¦ç¨‹åº¦å’Œæ¸…æ™°åº¦çš„å¯¹åº”å…³ç³»æ›²çº¿ï¼Œå¯¹å›¾åƒä¿¡æ¯å¤„ç†å™¨çš„æ€§èƒ½ä¾èµ–ç¨‹åº¦é«˜ã€‚

2. å¯¹ç„¦æ·±åº¦æ³• DFE

è¯¥æ–¹æ³•æ˜¯ä¸€ç§æœå¯»æ­£ç„¦å¹³é¢çš„æ–¹æ³•ï¼Œ é¦–å…ˆé€šè¿‡ç›¸æœºé‡‡é›†ä¸€ç³»åˆ—å›¾åƒï¼Œç„¶åå¯¹è·å–åˆ°çš„å›¾åƒï¼Œä½¿ç”¨å›¾åƒæ¸…æ™°åº¦ç®—æ³•åˆ†æå¤„ç†åï¼Œæ ¹æ®å¾—åˆ°çš„è¯„ä»·å€¼çš„å¤§å°ï¼Œæ§åˆ¶é©±åŠ¨è£…ç½®å¸¦åŠ¨ä½ç§»å¹³å°ï¼Œç§»åŠ¨åˆ°å›¾åƒæ¸…æ™°åº¦è¯„ä»·æ›²çº¿çš„æå€¼ç‚¹ä½ç½®ï¼Œç†è®ºä¸Šå°±å¯ä»¥è·å¾—æœ€æ¸…æ™°å›¾åƒï¼Œå®Œæˆæ•´ä¸ªå¯¹ç„¦è¿‡ç¨‹ã€‚ è¯¥æ–¹æ³•çš„å¯¹ç„¦é€Ÿåº¦æ¯”ç¦»ç„¦æ·±åº¦æ³•æ…¢ï¼Œä½†å¯¹ç„¦ç²¾åº¦æ¯”å®ƒé«˜ã€‚

è¿™ä¸¤ç§æ–¹å¼ç†è§£èµ·æ¥æœ‰ç‚¹å›°éš¾ï¼Œç®€å•æ¥è¯´å°±æ˜¯ç¦»ç„¦æ·±åº¦æ–¹æ³•æ˜¯ä»ç¦»ç„¦å›¾åƒä¸­è·å¾—æ¨¡ç³Šç¨‹åº¦å’Œæ·±åº¦ä¿¡æ¯ã€‚å¯¹ç„¦æ·±åº¦æ³•é¦–å…ˆéœ€è¦ä¸€ç³»åˆ—æ¨¡ç³Šç¨‹åº¦ä¸åŒçš„å›¾åƒï¼Œè®¡ç®—è¿™ä¸€ç³»åˆ—å›¾åƒçš„æ¸…æ™°åº¦è¯„ä»·å€¼ï¼Œå†æ ¹æ®è¿™äº›è¯„ä»·å€¼ï¼Œç¡®å®šåˆç„¦ä½ç½®ã€‚è¿™ä¸¤ç§æ–¹æ³•éƒ½æ¶‰åŠåˆ°æ¸…æ™°åº¦è¯„ä»·çš„é—®é¢˜ï¼Œæ‰€ä»¥å¦‚ä½•å¾—åˆ°æ¸…æ™°åº¦è¯„ä»·çš„æ‹Ÿåˆå‡½æ•°ï¼Œå°±è€ƒç ”å„å¤§å‚å®¶çš„åŠŸåº•äº†ã€‚æ¥ä¸‹æ¥ç®€è¦ç½—åˆ—ä¸€ä¸‹å¸¸ç”¨çš„å‡ ç§æ¸…æ™°åº¦è¯„ä»·å‡½æ•°ä½œä¸ºå‚è€ƒã€‚å…·ä½“å†…å®¹è¯·å¤§å®¶è‡ªè¡ŒæŸ¥é˜…ç›¸å…³æ–‡çŒ®ã€‚

åŸºäºç©ºåŸŸçš„è¯„ä»·å‡½æ•°

Brenner å‡½æ•°
æ¢¯åº¦å¹³æ–¹ï¼ˆEOGï¼‰ å‡½æ•°
ç°åº¦å·®åˆ†ç»å¯¹å€¼ä¹‹å’Œï¼ˆSMDï¼‰ å‡½æ•°
Variance å‡½æ•°
äºŒçº§æ¢¯åº¦å¹³æ–¹ï¼ˆTenengradï¼‰ å‡½æ•°
Laplace å‡½æ•°
åŸºäºé¢‘åŸŸçš„å¯¹ç„¦è¯„ä»·å‡½æ•°

å‚…åˆ©å¶å˜æ¢
ç¦»æ•£ä½™å¼¦å˜æ¢ï¼ˆDCTï¼‰
åŸºäºç°åº¦ç†µçš„å¯¹ç„¦è¯„ä»·å‡½æ•°
æ”¹è¿›åçš„ç°åº¦ç†µï¼ˆimproved-histï¼‰ å‡½æ•°
åŸºäºç©ºåŸŸçš„è¯„ä»·å‡½æ•°è®¡ç®—é‡è¾ƒå°ã€æ•ˆç‡é«˜ï¼Œç¼ºç‚¹æ˜¯å•è°ƒæ€§å·®ã€ æ­£ç„¦å¹³é¢é™„è¿‘çµæ•åº¦ä½ã€‚ åŸºäºé¢‘åŸŸçš„å¯¹ç„¦è¯„ä»·å‡½æ•°èƒ½çœŸå®ååº”å›¾åƒçš„æ¸…æ™°åº¦ï¼Œ ä½†æ˜¯å‚…é‡Œå¶å˜æ¢ä½¿å¾—è®¡ç®—é‡å¤§ã€‚ åŸºäºç°åº¦ç†µçš„å¯¹ç„¦è¯„ä»·å‡½æ•°å®ç°ç®€å•ï¼Œ ä½†æ˜¯å¯¹ç„¦ç²¾åº¦ä½ã€ æŠ—å¹²æ‰°èƒ½åŠ›å·®ã€‚ ä»¥ä¸Šä¸‰å¤§ç±»å¸¸ç”¨çš„å¯¹ç„¦è¯„ä»·å‡½æ•°å„æœ‰ä¼˜ç¼ºç‚¹ï¼Œéœ€è¦æ ¹æ®å®é™…åœºæ™¯è¿›è¡Œé€‰æ‹©å’Œä¼˜åŒ–ã€‚


çº¢å¤–æµ‹è·ä»ªçš„åˆ†ç±»æœ‰æ¿€å…‰çº¢å¤–ï¼Œçº¢å¤–å’Œè¶…å£°æ³¢ä¸‰ç§ï¼Œçº¢å¤–æµ‹è·ä»ªå’Œè¶…å£°æ³¢æµ‹è·ä»ªç”±äºæµ‹é‡è·ç¦»æœ‰é™ï¼Œæµ‹é‡ç²¾åº¦å¾ˆä½ç›®å‰åŸºæœ¬å·²ç»è¢«æ·˜æ±°äº†ã€‚

è¶…å£°æ³¢æµ‹è·çš„åŸç†æ˜¯åˆ©ç”¨è¶…å£°æ³¢åœ¨ç©ºæ°”ä¸­çš„ä¼ æ’­é€Ÿåº¦ä¸ºå·²çŸ¥ï¼Œæµ‹é‡å£°æ³¢åœ¨å‘å°„åé‡åˆ°éšœç¢ç‰©åå°„å›æ¥çš„æ—¶é—´ï¼Œæ ¹æ®å‘å°„å’Œæ¥æ”¶çš„æ—¶é—´å·®è®¡ç®—å‡ºå‘å°„ç‚¹åˆ°éšœç¢ç‰©çš„å®é™…è·ç¦»ã€‚è¶…å£°æ³¢æµ‹è·ä»ªç”±è¶…å£°æ³¢å‘ç”Ÿç”µè·¯ã€è¶…å£°æ³¢æ¥æ”¶æ”¾å¤§ç”µè·¯ã€è®¡æ•°å’Œæ˜¾ç¤ºç”µè·¯ç»„æˆã€‚

ã€€ã€€1ã€ç²¾åº¦ä¸Šï¼Œè¶…å£°æ³¢æµ‹è·ä»ªçš„æµ‹é‡ç²¾åº¦æ˜¯å˜ç±³çº§çš„

ã€€ã€€2ã€æµ‹é‡èŒƒå›´ä¸Šï¼Œè¶…å£°æ³¢æµ‹è·ä»ªçš„æµ‹é‡èŒƒå›´é€šå¸¸åœ¨80ç±³ä»¥å†…ï¼Œè€Œæ‰‹æŒå¼æ¿€å…‰æµ‹è·ä»ªçš„æµ‹é‡èŒƒå›´æœ€é«˜å¯åˆ°200ç±³

ã€€ã€€3ã€è¶…å£°æ³¢æµ‹è·ä»ªå®¹æ˜“æŠ¥é”™ï¼Œç”±äºè¶…å£°æ³¢æµ‹è·ä»ªæ˜¯å£°æ³¢å‘å°„ï¼Œå…·æœ‰å£°æ³¢çš„æ‰‡å½¢å‘å°„ç‰¹æ€§ï¼Œæ‰€ä»¥å½“å£°æ³¢ç»è¿‡ä¹‹å¤„éšœç¢ç‰©è¾ƒå¤šæ—¶ï¼Œåå°„å›æ¥çš„å£°æ³¢è¾ƒå¤šï¼Œå¹²æ‰°è¾ƒå¤šï¼Œæ˜“æŠ¥é”™

ã€€ã€€4ã€è¶…å£°æ³¢æµ‹è·ä»ªçš„ä»·æ ¼ä»å‡ åå…ƒåˆ°å‡ ç™¾å…ƒ



å…ˆå¤§èƒ†çŒœæµ‹ä¸€ä¸‹ï¼Œé¢˜ä¸»æ‰€è¯´çš„çº¢å¤–æµ‹è·åº”è¯¥æ˜¯sharpçš„çº¢å¤–æµ‹è·ä¼ æ„Ÿå™¨ã€‚ä»¥åŠæ·˜å®ä¸Šç”±å‡ å—é’±åˆ°å‡ ç™¾å—ä¸ç­‰çš„è¶…å£°æ³¢ä¼ æ„Ÿå™¨ã€‚




ä¸‹é¢å¼€å§‹è®²è®²è¿™ä¸¤ç§ä¼ æ„Ÿå™¨å„è‡ªçš„æƒ…å†µå§ã€‚

ä¸Šé¢çº¢å¤–ä¼ æ„Ÿå™¨ä¸ºsharpåˆ¶ä½œï¼Œæ˜¯ä¼ æ„Ÿå™¨è¡Œä¸šæ¯”è¾ƒç‰›Bçš„å…¬å¸ï¼Œè´¨é‡æ¯”è¾ƒå¯é ä¸€è‡´æ€§å¥½ï¼Œè¶…å£°æ³¢ä¼ æ„Ÿå™¨ä¸€èˆ¬ä¸ºå›½å†…å…¬å¸è‡ªä¸»è®¾è®¡ï¼Œäº§å“è´¨é‡å‚å·®ä¸é½ï¼Œè¿™ä¹Ÿæ˜¯ä¸ºä»€ä¹ˆä»·æ ¼ä¹Ÿå·®è·ç”šå¤§ã€‚

çº¢å¤–ä¼ æ„Ÿå™¨æ˜¯ä»€ä¹ˆåŸç†å‘¢ï¼Ÿçœ‹ä¸‹å›¾ï¼Œ

å·¥ä½œåŸç†ï¼š

Sharpçš„çº¢å¤–ä¼ æ„Ÿå™¨éƒ½æ˜¯åŸºäºä¸€ä¸ªåŸç†ï¼Œä¸‰è§’æµ‹é‡åŸç†ã€‚çº¢å¤–å‘å°„å™¨æŒ‰ç…§ä¸€å®šçš„è§’åº¦å‘å°„çº¢å¤–å…‰æŸï¼Œå½“é‡åˆ°ç‰©ä½“ä»¥åï¼Œå…‰æŸ ä¼šåå°„å›æ¥ï¼Œå¦‚å›¾1æ‰€ç¤ºã€‚åå°„å›æ¥çš„çº¢å¤–å…‰çº¿è¢«CCDæ£€æµ‹å™¨æ£€æµ‹åˆ°ä»¥åï¼Œä¼šè·å¾—ä¸€ä¸ªåç§»å€¼Lï¼Œåˆ©ç”¨ä¸‰è§’å…³ç³»ï¼Œåœ¨çŸ¥é“äº†å‘å°„è§’åº¦aï¼Œåç§»è·Lï¼Œä¸­å¿ƒçŸ©Xï¼Œä»¥ åŠæ»¤é•œçš„ç„¦è·fä»¥åï¼Œä¼ æ„Ÿå™¨åˆ°ç‰©ä½“çš„è·ç¦»Då°±å¯ä»¥é€šè¿‡å‡ ä½•å…³ç³»è®¡ç®—å‡ºæ¥äº†ã€‚





å¯ä»¥çœ‹åˆ°ï¼Œå½“Dçš„è·ç¦»è¶³å¤Ÿè¿‘çš„æ—¶å€™ï¼ŒLå€¼ä¼šç›¸å½“å¤§ï¼Œè¶…è¿‡CCDçš„æ¢æµ‹èŒƒå›´ï¼Œè¿™æ—¶ï¼Œè™½ç„¶ç‰©ä½“å¾ˆè¿‘ï¼Œä½†æ˜¯ä¼ æ„Ÿå™¨åè€Œçœ‹ä¸åˆ° äº†ã€‚å½“ç‰©ä½“è·ç¦»Då¾ˆå¤§æ—¶ï¼ŒLå€¼å°±ä¼šå¾ˆå°ã€‚è¿™æ—¶CCDæ£€æµ‹å™¨èƒ½å¦åˆ†è¾¨å¾—å‡ºè¿™ä¸ªå¾ˆå°çš„Lå€¼æˆä¸ºå…³é”®ï¼Œä¹Ÿå°±æ˜¯è¯´CCDçš„åˆ†è¾¨ç‡å†³å®šèƒ½ä¸èƒ½è·å¾—è¶³å¤Ÿç²¾ç¡®çš„Lå€¼ã€‚è¦ æ£€æµ‹è¶Šæ˜¯è¿œçš„ç‰©ä½“ï¼ŒCCDçš„åˆ†è¾¨ç‡è¦æ±‚å°±è¶Šé«˜ã€‚


ç”±çº¢å¤–ä¼ æ„Ÿå™¨å·¥ä½œåŸç†å¯ä»¥å¾—å‡ºä¸€ä¸ªç¼ºç‚¹ï¼šæ£€æµ‹çš„æœ€å°è·ç¦»å¤ªå¤§ã€‚

å…¶å®ä¸å…‰æœ‰æœ€å°é™åˆ¶ï¼Œè¿˜å…·æœ‰éçº¿æ€§ï¼š




å†æ¥è¯´è¯´è¶…å£°æ³¢ä¼ æ„Ÿå™¨ã€‚

è¶…ç”Ÿæ³¢ä¼ æ„Ÿå™¨æ£€æµ‹è·ç¦»åŸç†æ˜¯å‘å‡ºè¶…å£°æ³¢å†æ£€æµ‹åˆ°å‘å‡ºçš„è¶…å£°æ³¢ï¼ŒåŒæ—¶æ ¹æ®å£°é€Ÿè®¡ç®—å‡ºç‰©ä½“çš„è·ç¦»ã€‚

æ ¹æ®å…¶åŸç†å¯ä»¥çŸ¥é“å…¶ç¼ºç‚¹æœ‰ä¸‹2ä¸ªï¼šå£°éŸ³çš„é€Ÿåº¦å—æ¸©åº¦å’Œé£å‘çš„å¹²æ‰°ï¼Œæœ‰å¯èƒ½è¢«å¸éŸ³é¢ç»™å¸æ”¶ã€‚

å…¶ä¼˜åŠ¿æŒ‰ç…§ç°åœ¨æŸå®ä¸Šé”€å”®çš„ä¹Ÿå¯ä»¥çœ‹è§ï¼šå¯ä»¥é€‰æ‹©çš„è¾“å‡ºæ–¹å¼å¤šå˜ï¼Œå‡ ä¹ä»€ä¹ˆåè®®éƒ½æœ‰ï¼Œä¸²å£ï¼ŒI2Cï¼ŒPWMï¼Œè½¯ä»¶è®¾å®šé˜ˆå€¼ä»€ä¹ˆçš„ã€‚æµ‹é‡è·ç¦»æ™®éæ¯”çº¢å¤–çš„è¿œï¼Œæœ€è¿‘æµ‹é‡è·ç¦»è¾ƒå°ã€‚



å·¥ä¸šæ£€æµ‹ï¼Œå›¾åƒç®—æ³•ï¼Œã€Šæ·±åº¦å­¦ä¹ æŠ€æœ¯å›¾åƒå¤„ç†å…¥é—¨ã€‹
çº¢å¤–æµ‹è·é€Ÿåº¦å¿«ï¼Œä½†æ˜¯å¤ªé˜³å…‰ä¸€æ¥ä½ å°±æµ‹ä¸å‡†äº†ï¼Œæˆ‘ä»¬æ›¾ç»åšè¿‡ä¸€ä¸ªé¿éšœå°è½¦ï¼Œå®¤å†…æ•ˆæœå¾ˆå¥½ï¼Œæ‹¿åˆ°å®¤å¤–ç›´æ¥å°±ä¸è·‘äº†ï¼Œåè€Œæ˜¯æœ‰éšœç¢ç‰©çš„æ—¶å€™æ‰è·‘ã€‚

åŸç†å±…ç„¶æ˜¯è¿™æ ·ï¼å› ä¸ºçº¢å¤–æµ‹è·çš„åŸç†æ˜¯ï¼Œä¸€ä¸ªçº¢å¤–LEDå‘å…‰ï¼Œç„¶åå¦å¤–ä¸€ä¸ªçº¢å¤–æ¥æ”¶ç®¡æµ‹çº¢å¤–å…‰çš„å¼ºåº¦ï¼å¤ªé˜³å…‰ä¸€æ¥å…‰çº¿å¼ºåº¦ç›´æ¥å°±æœ€å¤§äº†ï¼Œç„¶åæœ‰éšœç¢ç‰©ä¹‹åå…‰å¼ºæ‰ä¼šå˜ä½ã€‚åœ¨å®¤å†…å…‰çº¿å¼ºåº¦å’Œè·ç¦»æˆæ­£æ¯”ï¼Œåœ¨å®¤å¤–æœ‰ç‚¹æˆåæ¯”ï¼Œæ‰€ä»¥ç¨‹åºå°±è·‘ä¸å¯¹äº†ã€‚

å¦å¤–çº¢å¤–è¿˜æœ‰ä¸€ä¸ªç¼ºç‚¹ï¼Œå°±æ˜¯ä¸åŒçš„é¢è¿”å›çš„å…‰çº¿å¼ºåº¦ä¸ä¸€æ ·ï¼Œé»‘è‰²è¿”å›çš„æ•°æ®è¦æ¯”ç™½è‰²åº•è®¸å¤šï¼Œæ‰€ä»¥å…‰çº¿å¼ºåº¦å’Œè·ç¦»ä¸ä¸€å®šå®Œå…¨æˆæ­£æ¯”ï¼Œææ–™ä¸€æ¢ï¼Œå¯¹åº”çš„å…³ç³»ä¹Ÿå°±æ¢äº†ã€‚



è¶…å£°æ³¢æµ‹è·ä¹Ÿæœ‰è®¸å¤šç¼ºç‚¹ã€‚

æœ€å¤§çš„ä¸€ä¸ªç¼ºç‚¹å°±æ˜¯æ…¢ï¼Œå¦‚æœä½ è¦æµ‹3ç±³çš„è·ç¦»ï¼Œå…ˆä¸è€ƒè™‘å£°æ³¢èƒ½ä¸èƒ½ä¼ å›æ¥ï¼Œé¦–å…ˆ3*2/340å°±è¶…è¿‡10msäº†ï¼Œè·ç¦»å†è¿œä¸€ç‚¹å°±æ²¡æ³•æµ‹äº†ï¼Œå£°éŸ³ä¼ ä¸å›æ¥ã€‚

å…¶æ¬¡è¿˜æœ‰ä¸€ä¸ªç¼ºç‚¹å°±æ˜¯å¿…é¡»æ˜¯å¹³é¢ï¼Œè¯·è‡ªè¡Œè„‘è¡¥45åº¦é¢æ¥æ”¶åˆ°å£°éŸ³ä¹‹åæ‰“åˆ°ä¸€è¾¹çš„åœºæ™¯ã€‚

æœ€åå†æ¥ä¸€ä¸ªç¼ºç‚¹ï¼Œä¸èƒ½åŒæ—¶æ£€æµ‹ï¼Œå› ä¸ºå£°éŸ³æ˜¯å¯ä»¥äº’ç›¸å¹²æ‰°çš„ï¼Œåªèƒ½è½®è¯¢ï¼Œä¸€ä¸ªæ¥ä¸€ä¸ªå»æ£€æµ‹ã€‚
 
[70] P. Memmolo, C. Distante, M. Paturzo, A. Finizio, P. Ferraro, B. Javidi, Opt. Lett. 2011, 36, 
1945-1947. 
[71] P. Ferraro, G. Coppola, S. De Nicola, A. Finizio, G. Pierattini, Opt. Lett. 2003, 28, 1257-1259. 
[72] Y. Zhang, H. Wang, Y. Wu, M. Tamamitsu, A. Ozcan, Opt. Lett. 2017, 42, 3824-3827. 
[73] M. Lyu, C. Yuan, D. Li, G. Situ, Appl. Opt. 2017, 56, F152-F157. 
[74] W. Li, N. C. Loomis, Q. Hu, C. S. Davis, JOSA A 2007, 24, 3054-3062. 
[75] F. Dubois, C. Schockaert, N. Callens, C. Yourassowsky, Opt. Express 2006, 14, 5895-5908. 
[76] A. Thelen, J. Bongartz, D. Giel, S. Frey, P. Hering, JOSA A 2005, 22, 1176-1180. 
[77] P. Memmolo, M. Paturzo, B. Javidi, P. A. Netti, P. Ferraro, Opt. Lett. 2014, 39, 4719-4722. 
[78] Z. Ren, N. Chen, E. Y. Lam, Opt. Lett. 2017, 42, 1720-1723. 
[79] K. Bahrami, A. C. Kot, IEEE Signal Process. Lett. 2014, 21, 751-755. 
[80] L. Li, W. Xia, W. Lin, Y. Fang, S. Wang, IEEE Trans. Multimed. 2016, 19, 1030-1040. 
[81] Y. Liu, K. Gu, G. Zhai, X. Liu, D. Zhao, W. Gao, J. Vis. Commun. Image Represent. 2017, 
46, 70-80. 
[82] A. Liu, W. Lin, M. Narwaria, IEEE Trans. Image Process. 2011, 21, 1500-1512. 
[83] J. Guan, W. Zhang, J. Gu, H. Ren, J. Vis. Commun. Image Represent. 2015, 29, 1-7. 
[84] G. Gvozden, S. Grgic, M. Grgic, J. Vis. Commun. Image Represent. 2018, 50, 145-158. 
[85] R. Hassen, Z. Wang, M. M. Salama, IEEE Trans. Image Process. 2013, 22, 2798-2810. 
[86] A. Leclaire, L. Moisan, Journal of Mathematical Imaging and Vision 2015, 52, 145-172. 
[87] N. D. Narvekar, L. J. Karam, IEEE Trans. Image Process. 2011, 20, 2678-2683. 
[88] A. JimÃ©nez, G. Bueno, G. CristÃ³bal, O. DÃ©niz, D. Toomey, C. Conway, Optics, Photonics 
and Digital Technologies for Imaging Applications IV, SPIE, 2016, 98960S 
[89] M. S. Hosseini, J. A. Brawley-Hayes, Y. Zhang, L. Chan, K. N. Plataniotis, S. Damaskinos, 
IEEE Trans. Med. Imaging 2019, 39, 62-74. 
[90] L. Kang, P. Ye, Y. Li, D. Doermann, Proceedings of the IEEE conference on computer vision 
and pattern recognition, IEEE, 2014, 1733-1740 
[91] S. Yu, S. Wu, L. Wang, F. Jiang, Y. Xie, L. Li, PLoS One 2017, 12. 
[92] C. Senaras, M. K. K. Niazi, G. Lozanski, M. N. Gurcan, PLoS One 2018, 13. 
[93] G. Campanella, A. R. Rajanna, L. Corsale, P. J. SchÃ¼ffler, Y. Yagi, T. J. Fuchs, Comput. Med. 
Imaging Graph. 2018, 65, 142-151. 
[94] S. J. Yang, M. Berndl, D. M. Ando, M. Barch, A. Narayanaswamy, E. Christiansen, S. Hoyer, 
C. Roat, J. Hung, C. T. Rueden, BMC Bioinformatics 2018, 19, 77. 
[95] Y. Liron, Y. Paran, N. Zatorsky, B. Geiger, Z. Kam, J. Microsc. 2006, 221, 145-151. 
[96] G. Reinheimer, US 3,721,827, 1973. 
[97] M. Sato, J. Matsuno, US 5,530,237, 1996. 
[98] Y. Yonezawa, US 5,483,079, 1996. 
[99] K. Ito, T. Musha, K. Kato, US 4,422,168, 1983. 
[100] H. Noda, S. Dosaka, H. Kurosawa, US 5,317,142, 1994. 
[101] P. Kramer, G. Bouwhuis, P. E. Day, US 3,876,841, 1975. 
[102] C. H. Velzel, P. F. Greve, US 4,074,314, 1978. 
This article is protected by copyright. All rights reserved.
 
hunting or errors in the focus drift correction system. Plastic tissue culture dishes are also not 
recommended, as the boundary surface may not be detectable due to insufficient offset111. 
3.3. Low-coherence interferometry with oblique illumination  
The idea of using optical coherence tomography (OCT) for autofocusing was proposed in a patent 
by Wei and Hellmuth in 1996112. The general concept is to locate the sample position using the 
axial depth reflectivity profile called A-scan, which contains scattering information of sample 
structures along the axial direction. In the original patent, an on-axis configuration is used to 
perform autofocusing of an ophthalmologic surgical microscope. However, it is not suitable for 
high-resolution imaging of tissue slides covered by glass. The main difficulty is the overlap 
between the large signal reflected by glass surfaces and the weak signal reflected from the sample. 
Locating the sample position with submicron accuracy is challenging given the large signals 
reflected from the glass surfaces.  
 
Figure 5. Low-coherence interferometry for reflective real-time autofocusing. A superluminescent diode is used as a 
low-coherence light source. The light illuminates the sample from a tilted incident angle. As such, most reflected light 
from the glass surface will not be coupled back to the interferometry system. The axial depth reflectivity profile (i.e., 
A-scan) is measured using a spectrometer. The recovered sample position is used to move the z stage or the objective 
lens. Adapted from Ref. 113.  
 
 This article is protected by copyright. All rights reserved.
Autofocusing technologies for whole slide imaging and automated microscopy 
Zichao Bian1,3, Chengfei Guo1,3, Shaowei Jiang,1,*, Jiakai Zhu1, Ruihai Wang1, Pengming Song2, 
Zibang Zhang1, Kazunori Hoshino1, and Guoan Zheng1,* 
1University of Connecticut, Department of Biomedical Engineering, Storrs, CT, 06269, USA 
2University of Connecticut, Department of Electrical and Computer Engineering, Storrs, CT, 
06269, USA 
3These authors contributed equally to this work 
*E-mail: shaowei.jiang@uconn.edu (S. J.) or guoan.zheng@uconn.edu (G. Z.)  
Abstract: Whole slide imaging (WSI) has moved digital pathology closer to diagnostic practice 
in recent years. Due to the inherent tissue topography variability, accurate autofocusing remains a 
critical challenge for WSI and automated microscopy systems. The traditional focus map 
surveying method is limited in its ability to acquire a high degree of focus points while still 
maintaining high throughput. Real-time approaches decouple image acquisition from focusing, 
thus allowing for rapid scanning while maintaining continuous accurate focus. This work reviews 
the traditional focus map approach and discusses the choice of focus measure for focal plane 
determination. It also discusses various real-time autofocusing approaches including reflective-
based triangulation, confocal pinhole detection, low-coherence interferometry, tilted sensor 
approach, independent dual sensor scanning, beam splitter array, phase detection, dual-LED 
illumination, and deep-learning approaches. The technical concepts, merits, and limitations of 
these methods are explained and compared to those of a traditional WSI system. This review may 
provide new insights for the development of high-throughput automated microscopy imaging 
systems that can be made broadly available and utilizable without loss of capacity.   
 
Keywords: whole slide imaging, digital pathology, focus quality, focus map, deep learning. 
 
 
 
 
 
 This article is protected by copyright. All rights reserved.
This article has been accepted for publication and undergone full peer review but has 
not been through the copyediting, typesetting, pagination and proofreading process 
which may lead to differences between this version and the Version of Record. Please 
cite this article as doi: 10.1002/jbio.202000227
 
[103] R. Jorgens, B. Faltermeier, US 4,958,920, 1990. 
[104] O. Mueller, US 4,025,785, 1977. 
[105] Q. Li, L. Bai, S. Xue, L. Chen, Opt. Eng. 2002, 41, 1289-1294. 
[106] C.-S. Liu, S.-H. Jiang, Meas. Sci. Technol. 2013, 24, 105101. 
[107] C.-S. Liu, Y.-C. Lin, P.-H. Hu, Microsyst. Technol. 2013, 19, 1717-1724. 
[108] C.-S. Liu, S.-H. Jiang, Appl. Phys B 2014, 117, 1161-1171. 
[109] C.-S. Liu, Z.-Y. Wang, Y.-C. Chang, Appl. Phys B 2015, 121, 69-80. 
[110] C.-S. Liu, S.-H. Jiang, Opt. Lasers Eng. 2015, 66, 294-300. 
[111] J. S. Silfies, E. G. Lieser, S. A. Schwartz, M. W. Davidson, Nikon Perfect Focus System 
(PFS), https://www.microscopyu.com/applications/live-cell-imaging/nikon-perfect-focus-system. 
[112] J. Wei, T. Hellmuth, US 5,493,109, 1996. 
[113] A. Cable, J. Wollenzin, R. Johnstone, K. Gossage, J. S. Brooker, J. Mills, J. Jiang, D. 
Hillmann, US 9,869,852, 2018. 
[114] R. R. McKay, V. A. Baxi, M. C. Montalto, J. Pathol. Inform. 2011, 2. 
[115] T. VirÃ¡g, A. LÃ¡szlÃ³, B. MolnÃ¡r, A. Tagscherer, V. S. Varga, US 7,663,078, 2010. 
[116] P. Prabhat, S. Ram, E. S. Ward, R. J. Ober, IEEE Transactions on Nanobioscience 2004, 3, 
237-242. 
[117] S. Abrahamsson, J. Chen, B. Hajj, S. Stallinga, A. Y. Katsov, J. Wisniewski, G. Mizuguchi, 
P. Soule, F. Mueller, C. D. Darzacq, Nat. Methods. 2013, 10, 60-63. 
[118] A. Descloux, K. GruÃŸmayer, E. Bostan, T. Lukes, A. Bouwens, A. Sharipov, S. 
Geissbuehler, A.-L. Mahul-Mellier, H. Lashuel, M. Leutenegger, Nat. Photon. 2018, 12, 165-172. 
[119] S. Xiao, H. Gritton, H.-a. Tseng, D. Zemel, X. Han, J. Mertz, bioRxiv 2020. 
[120] R.-T. Dong, U. Rashid, J. Zeineh, US 2005/0089208A1, 2005. 
[121] B. Hulsken, S. Stallinga, US 10,353,190, 2019. 
[122] B. Hulsken, US 10,091,445, 2018. 
[123] B. Hulsken, US 9,578,227, 2017. 
[124] B. Hulsken, US 9,910,258, 2018. 
[125] B. Hulsken, S. Stallinga, US 10,365,468, 2019. 
[126] J. P. Vink, B. Hulsken, M. Wolters, M. B. Van Leeuwen, S. H. Shand, US 10,623,627, 2020. 
[127] Y. Zou, G. J. Crandall, A. Olson, US 9,841,590, 2017. 
[128] A. Olson, K. Saligrama, Y. Zou, P. Najmabadi, US 10,459,193, 2020. 
[129] J. H. Price, US 5,932,872, 1999. 
[130] A. Kinba, M. Hamada, H. Ueda, K. Sugitani, H. Ootsuka, US 5,597,999, 1997. 
[131] K. Guo, J. Liao, Z. Bian, X. Heng, G. Zheng, Biomed. Opt. Express 2015, 6, 3210-3216. 
[132] J. Liao, L. Bian, Z. Bian, Z. Zhang, C. Patel, K. Hoshino, Y. C. Eldar, G. Zheng, Biomed. 
Opt. Express 2016, 7, 4763-4768. 
[133] L. Silvestri, M. C. Muellenbroich, I. Costantini, A. P. Di Giovanna, L. Sacconi, F. S. Pavone, 
bioRxiv 2017, 170555. 
[134] J. Liao, Z. Wang, Z. Zhang, Z. Bian, K. Guo, A. Nambiar, Y. Jiang, S. Jiang, J. Zhong, M. 
Choma, G. Zheng, J. Biophotonics 2018, 11, e201700075. 
This article is protected by copyright. All rights reserved.
 
One solution to this problem is to substantially reduce the light reflected from glass surfaces 
while keeping the sample scattering light relatively unchanged. Figure 5 demonstrates such a 
solution by using an off-axis configuration, where the light illuminates the sample at a tilted 
incident angle113. As such, the light directly reflected from the glass surface will not be coupled 
back to the interferometry system. In Figure 5, a broadband superluminescent diode is used as the 
low-coherence light source. The axial depth reflectivity profile (i.e., A-scan) is measured using a 
spectrometer in a Fourier-domain OCT setup. The sample position can be calculated by performing 
a Fourier transform of the captured spectrum and used to move the objective lens to the in-focus 
position.   
Since OCT is sensitive to refraction index variations within the sample, this approach can 
handle transparent samples that may be challenging for the traditional focus map approach. The 
disadvantages, perhaps, are the complicated Fourier-domain OCT setup, the precise optical 
alignment, and the high maintenance of the system.  
4. Real-time image-based autofocusing  
The pre-scan focus map approach requires the acquisition of a z-stack for each focus point. The 
sample needs to be scanned to different x-y positions for acquiring multiple z-stacks to generate 
the focus map. In many WSI systems, the overhead time for generating the focus map is a 
substantial portion of the total scanning time. In this section, we will discuss several real-time 
image-based autofocusing approaches without the need for generating the focus map.   
4.1. Independent dual sensor scanning  
The traditional focus map approach uses the same image sensor to both survey the focus and 
acquire the image. In between two image acquisitions, there is a certain amount of â€˜dead timeâ€™ to 
read out the data to the memory. As a result, the main camera cannot be used to survey the focus 
during this â€˜dead timeâ€™. An independent secondary image sensor has been proposed to survey the 
focus in parallel6, 114.  
 This article is protected by copyright. All rights reserved.
 
Short descriptive and popular text: A fundamental challenge with automated microscopy and 
high-throughput imaging has been the ability to acquire high-quality, in-focus images at high 
speed. This review article discusses different autofocusing approaches for automated microscopy 
and whole slide imaging (WSI). The technical concepts, merits, and limitations of these methods 
are explained and compared to those of a traditional WSI system. 
1. Introduction 
The process of analyzing pathology slides using an optical microscope has remained relatively 
unchanged until recently. In a regular process, pathologists move the microscope stage to different 
positions to identify areas of interest, which can be further analyzed by switching to a higher 
magnification objective lens. The focusing of the slide is manually performed using the focus knob 
of the microscope platform. Although this traditional slide reviewing process remains the gold 
standard in diagnosing a large number of diseases including almost all types of cancers, it is highly 
subjective on the other hand: different pathologists may arrive at different conclusions and the 
same person may also give different conclusions at different time points. In terms of workflow 
efficiency, this process is labor-intensive and can be easily disrupted when a pathologist bumps a 
slide to a high magnification objective lens1. Similarly, it can be disrupted when the pathologist 
switches to a different objective lens and performs manual focusing of the slide. After the 
reviewing process, the slides must be kept accessible, clean and protected, creating additional 
storage and labor demands1, 2. 
Since the current slide reviewing process is based on subjective opinions of pathologists, there 
is a need for quantitative and streamlined assessment of histology slides. Quantitative 
characterization of pathology imagery is not only important for reducing inter- and intra-observer 
variations in diagnosis but also to better understand the biological mechanisms of the disease 
process3. Recent clinical guidelines have begun to require quantitative evaluations as part of the 
effort towards better patient risk stratification4. For example, breast cancer staging requires the 
counting of mitotic cells.  
 This article is protected by copyright. All rights reserved.
 
[135] J. Liao, S. Jiang, Z. Zhang, K. Guo, Z. Bian, Y. Jiang, J. Zhong, G. Zheng, J. Biomed. Opt. 
2018, 23, 066503. 
[136] J. Liao, Y. Jiang, Z. Bian, B. Mahrou, A. Nambiar, A. W. Magsam, K. Guo, S. Wang, Y. ku 
Cho, G. Zheng, Opt. Lett. 2017, 42, 3379-3382. 
[137] S. Jiang, Z. Bian, X. Huang, P. Song, H. Zhang, Y. Zhang, G. Zheng, Quant Imaging Med 
Surg 2019, 9, 823-831. 
[138] C. Guo, Z. Bian, S. Jiang, M. Murphy, J. Zhu, R. Wang, P. Song, X. Shao, Y. Zhang, G. 
Zheng, Opt. Lett. 2020, 45, 260-263. 
[139] C. Belthangady, L. A. Royer, Nat. Methods. 2019, 1-11. 
[140] T. R. Dastidar, R. Ethirajan, Biomed. Opt. Express 2020, 11, 480-491. 
[141] S. Jiang, J. Liao, Z. Bian, K. Guo, Y. Zhang, G. Zheng, Biomed. Opt. Express 2018, 9, 1601-
1612. 
[142] Q. Li, X. Liu, K. Han, C. Guo, X. Ji, X. Wu, arXiv preprint arXiv:2003.06630 2020. 
[143] Y. Luo, L. Huang, Y. Rivenson, A. Ozcan, arXiv preprint arXiv:2003.09585 2020. 
[144] H. Pinkard, Z. Phillips, A. Babakhani, D. A. Fletcher, L. Waller, Optica 2019, 6, 794-797. 
[145] Y. Rivenson, Z. GÃ¶rÃ¶cs, H. GÃ¼naydin, Y. Zhang, H. Wang, A. Ozcan, Optica 2017, 4, 1437-
1443. 
[146] A. Shajkofci, M. Liebling, 2018 25th IEEE International Conference on Image Processing 
(ICIP), IEEE, 2018, 3818-3822 
[147] A. Shajkofci, M. Liebling, arXiv preprint arXiv:2001.00667 2020. 
[148] Y. Wu, Y. Rivenson, H. Wang, Y. Luo, E. Ben-David, L. A. Bentolila, C. Pritz, A. Ozcan, 
Nat. Methods. 2019, 16, 1323-1331. 
[149] Z. Ren, Z. Xu, E. Y. Lam, Optica 2018, 5, 337-344. 
[150] L. Wei, E. Roberts, Sci. Rep. 2018, 8, 1-10. 
[151] O. Ronneberger, P. Fischer, T. Brox, International Conference on Medical image computing 
and computer-assisted intervention, Springer, 2015, 234-241 
[152] P. Isola, J.-Y. Zhu, T. Zhou, A. A. Efros, Proceedings of the IEEE conference on computer 
vision and pattern recognition, 2017, 1125-1134 
 
 This article is protected by copyright. All rights reserved.
 
 
Figure 6. Independent dual sensor scanning for real-time image-based autofocusing. (a) The optical scheme, where a 
high-speed focusing camera is used to survey the focus in parallel with the main camera. (b) The focusing sensor 
acquires three autofocus images, each at a slightly different focal plane. The system calculates the optimal focus 
position and moves the sample to that focal plane, where the main camera takes a high-resolution image. (c) The stage 
is in continuous motion during this process and the captured three images only share a small region of overlap. 
Modified from Ref. 6. 
 
 Figure 6 shows the principle and operation of this concept. In Figure 6(a), an independent 
camera, termed focusing sensor, is used to survey the focus while the main camera captures the 
high-resolution sample images. During the scanning process, the stage is in continuous motion and 
the motion blur is eliminated by using short pulses of light during imaging. As shown in Figure 
6(b), the focusing sensor acquires three autofocus images, each at a slightly different focal plane. 
Based on these three images, the system calculates the optimal focus position and moves the 
sample to that focal plane25, where the main camera takes a high-resolution image. When the main 
camera is reading out image data, the autofocusing is repeated for the next tile position to predict 
its optimal focal plane ahead. Since the stage is in continuous motion during this process, the 
captured three focus images only share a small region of overlap (Figure 6(c)). Only the 
overlapping region is used to calculate the correct focal position. The autofocusing performance 
This article is protected by copyright. All rights reserved.
 
A whole slide imaging (WSI) system is designed to replace the traditional microscope for 
quantitative and streamlined slide reviewing. It was first developed based on a robotic microscope 
platform in 1990s5. The essential components of a WSI system include the following: 1) a 
microscope with objective lenses, 2) robotics to move slides, 3) one or more image sensors for 
image acquisition and autofocusing, and 4) software for management. In the acquisition process, 
a typical WSI system captures hundreds of high-resolution images that are subsequently aligned 
and stitched together to create a complete and seamless representation of the original whole tissue 
section6. The stitched whole slide image can provide a digital equivalent of the original glass slide 
on the microscope. The pathologists can then view, navigate, change magnification, and annotate 
the virtual slide with speed and ease. Digital pathology using WSI is now advancing into clinical 
workflow for better and faster predication, diagnosis, and prognosis of cancers and other diseases1. 
A major milestone was accomplished in 2017 when the U.S. Food and Drug Administration 
approved the first WSI scanner for primary diagnostic use in the U.S.7, 8. The new generation of 
pathologists trained on digital pathology promises further growth of the field in the coming 
decades. 
Another driving force for the development of digital pathology is the recent advancement of 
artificial intelligence (AI) in medical diagnosis9-13. In particular, deep-learning approaches have 
been demonstrated for automated analysis of microscopic pathology images with performance 
comparable to that by human experts14-18. An augmented reality microscope has also recently been 
developed to provide real-time integration of AI in the slide inspection process15. In this augmented 
reality microscope platform, two modules are attached to a regular upright microscope. The first 
module is a digital camera that captures high-resolution images of the same field of view as one 
observes through the eyepiece ports. The second module is a microdisplay that projects digital 
information into the eyepiece ports. In a typical implementation, the captured image from the 
camera will be processed by a deep learning algorithm to produce a heatmap that predicts tumor 
probability. The outline of the predicted tumor regions will then be projected to the eyepiece ports 
This article is protected by copyright. All rights reserved.
 
of this system has been validated with various tissue sections114. The average focusing error is 
~0.30 Âµm for the continuous motion scheme. Around 95% of tiles fall within the systemâ€™s depth 
of field. 
4.2. Beam splitter array    
In the independent dual sensor scanning scheme discussed above, multiple images are acquired to 
calculate the focus position when the sample is moved to different focal planes. In a patent 
published in 2010, Virag et al. proposed to use a beam splitter array to allow capturing images at 
different focal planes on the same image sensor115. Figure 7 shows the imaging setup, where the 
focusing optics comprises a main imaging camera and a secondary focusing camera. A beam 
splitter array is used to split and direct the light beam to different regions of the focusing sensor. 
As such, the system can capture images at multiple focal planes at the same time. The 45-degree 
semi-reflective surfaces in the beam splitter array are chosen to assure that all beams reflected by 
the surfaces have roughly the same intensities. With the image captured by the focusing sensor, a 
certain focus measure and fitting model can be used to infer the optimal focus position. In 
additional to autofocusing, this scheme can also be modified for real-time multiplane 
microscopy116-119, which finds important applications in volumetric imaging of biological samples.   
 
This article is protected by copyright. All rights reserved.
 
via the microdisplay. As such, the pathologists can observe the original specimen overlaid with 
the AI-assisted information through the eyepiece ports.     
A fundamental challenge with WSI, automated microscopy, and augmented reality microscopy 
has been the ability to acquire high-quality, in-focus images at high speed. For a high numerical 
aperture (NA) objective lens, the depth of field is on the orders of 1 Âµm. The small depth of field 
poses a difficulty to track the axial topography variations that inherently exist in solid tissue 
samples6. If the specimen is not placed within the depth of field of the objective lens, the image 
quality of the acquisition will be degraded, leading to rescanning and workflow delays. Several 
studies have implicated poor focus as the main culprit for poor image quality in WSI19-21. For 
augmented reality microscopy, defocus blur can occur to the captured images due to the optical 
path length difference between the eyepiece ports and the camera port. This optical path length 
difference varies for different objective lenses. As a result, it is challenging to maintain the in-
focus position for the camera when the pathologist keeps switching to different objective lenses in 
the slide reviewing process. Furthermore, some pathologists may have certain vision conditions 
such as myopia. Instead of adjusting the diopter on the eyepieces, they may prefer to adjust the 
focus knob to bring the sample into focus for their eye observation. The captured image through 
the camera port, on the other hand, will be out-of-focus due to the introduced optical path length 
difference. To address these challenges in augmented reality microscopy, a real-time autofocusing 
module is needed to acquire high-quality, in-focus images at high speed.    
Here we review and discuss different autofocusing techniques for WSI and automated 
microscopy in general. A list of commercially-available WSI scanners and automated microscopy 
systems are provided in Table 1. The employed autofocusing techniques are listed in the last 
column and they can be categorized into three groups: 1) pre-scan focus map approach, 2) real-
time reflective autofocusing, and 3) real-time image-based autofocusing. In the following, we will 
first review the traditional pre-scan focus map approach in Section 2. We will discuss the choice 
of different focus measures for determining the best focal position. In Section 3, we will review 
the reflective autofocusing approaches, including intensity detection via confocal pinhole, 
This article is protected by copyright. All rights reserved.
 
Figure 7. Beam splitter array for real-time image-based autofocusing. A beam splitter array is used to split and direct 
the light beam to different regions on the focusing sensor. As such, the system can capture images at multiple focal 
planes for determining the optimum focus position. Modified from Ref. 115.     
4.3. Tilted sensor   
The tilted sensor approach uses a tilted focusing sensor to image an oblique cross-section of the 
sample. The optimum focus position can be inferred by locating the peak of the contrast curve in 
real time. The concept of this approach was originally proposed in a patent by Dong et al. in 
2005120. There are some further refinements and developments of this original concept by 
Philips121-126 and Leica127, 128. Arguably, it is one of the most successful autofocusing technologies 
employed in existing commercially available WSI systems. 
 
 
This article is protected by copyright. All rights reserved.
 
triangulation with oblique illumination, and low-coherence interferometry. In Section 4, we will 
review and discuss various real-time image-based autofocusing approaches, including tilted 
sensor, independent dual sensor scanning, beam splitter array, phase detection, dual-LED 
illumination, and deep-learning approaches. The technical concepts, merits, and limitations of 
these methods are explained and compared to those of a traditional focus map approach. In Section 
5, we will summarize our discussion and provide perspectives for future development. This review 
may provide new insights for the development of high-throughput automated microscopy systems 
that can be made broadly available and utilizable without loss of capacity. 
 
 Vendor Model Imaging 
mode 
 Slide 
capacity 
 Scanning speed 
(15 mm x 15 
mm region) 
 Sensor type Autofocusing 
method 
Zeiss Axio Scan.Z1 Brightfield, 
Fluorescence 
 12 or 100 
slides 20Ã— 240 sec/slide 3 CCD sensor, 
sCMOS sensor Focus map 
Olympus VS200 
 Brightfield, 
Darkfield, 
 Phase contrast, 
Polarization, 
Fluorescence 
 210 slides 20Ã—: 80 sec/slide Area sensor Focus map 
Hamamatsu NanoZoomer 
S360 Brightfield 360 slides 20Ã—: ~30 sec/slide 
40Ã—: ~30 sec/slide TDI sensor Focus map 
Huron TissueScope 
LE120 Brightfield 120 slides 20Ã—: <60 sec/slide Area sensor Focus map 
Ventana iScan HT Brightfield 360 slides 20Ã—: <45 sec/slide 
40Ã—: <72 sec/slide 
 Information 
not available Focus map 
Leica 
 Aperio AT2 
DX Brightfield 6 or 400 
slides 20Ã—: <72 sec/slide TDI sensor Focus map 
Aperio GT 450 Brightfield 450 slides 40Ã—: 32 sec/slide TDI sensor Tilted sensor 
3DHistech 
 Pannoramic 
1000 Brightfield 1000 
slides 
 20Ã—:<60 sec/slide 
40Ã—:<60 sec/slide Area sensor Focus map 
Pannoramic 
250 Flash III 
 Brightfield, 
Fluorescence 250 slides 20Ã—: 35 sec/slide 
40Ã—: 95 sec/slide 
 3 CCD sensor, 
sCMOS sensor Focus map 
Philips Ultra fast 
scanner Brightfield 300 slides 40Ã—: 60 sec/slide TDI sensor Tilted sensor 
Nikon Eclipse Ti2 
(Perfect Focus) 
 Brightfield, 
 Phase contrast, 
Fluorescence 
 1 slide Not available Area sensor 
 Triangulation 
with oblique 
illumination 
Olympus IXplore 
(TruFocus) 
 Brightfield, 
 Phase contrast, 
Fluorescence 
 1 slide Not available Area sensor 
 Triangulation 
with oblique 
illumination 
Thorlabs EV103 Brightfield, 
Fluorescence 4 slides 20Ã—: <70 sec/slide 
40Ã—:<200sec/slide TDI sensor Low-coherence 
interferometry 
This article is protected by copyright. All rights reserved.
 
Figure 8. Tilted sensor for real-time image-based autofocusing. (a) The optical scheme, where a tilted sensor is used 
to infer the optimum focus position during the scanning process. (b) The overlapping position between the focusing 
sensor and the parfocal imaging plane is termed â€˜parfocal pointâ€™. (c) Contrast curve for determining the optimal focus 
position. The pixel distance (Î”N) between the parfocal point and the peak contrast point indicates a physical distance 
by which one needs to adjust the objective lens for optimal focusing. Modified from Ref. 127.  
 
 Figure 8 shows the principle and operation of the tilted sensor concept. In Figure 8(a), the 
focusing sensor is tilted at Î¸ angle with respect to the parfocal image plane. The imaging and 
focusing sensors can be either 2D area sensors or 1D linear sensors. The overlapping position 
between the focusing sensor and the parfocal imaging plane is termed â€˜parfocal pointâ€™ in Figure 
8(b). The focusing range is determined by Zrange. With a larger tilted angle, a longer focusing range 
can be expected.  
During the scanning process, both sensors capture images of the sample. For each pixel of the 
captured data, a contrast value can be determined based on the surrounding pixel values. Consider 
a 1D image data ğ¼ğ¼(ğ‘¥ğ‘¥) as an example, the contrast value ğ¶ğ¶(ğ‘¥ğ‘¥) can be calculated via ğ¶ğ¶(ğ‘¥ğ‘¥) =
âˆ‘ |ğ¼ğ¼(ğ‘¥ğ‘¥) âˆ’ ğ¼ğ¼(ğ‘¥ğ‘¥ âˆ’ ğ‘šğ‘š)|ğ‘ ğ‘ =ğ‘€ğ‘€ğ‘ ğ‘ =âˆ’ğ‘€ğ‘€ , where m define the surrounding range for the calculation. A contrast 
curve can then be obtained by dividing the focusing sensor contrast value ğ¶ğ¶ğ‘“ğ‘“ğ‘Ÿğ‘Ÿğ¿ğ¿ğºğºğºğº by the imaging 
sensor contrast value ğ¶ğ¶ğºğºğ‘ ğ‘ ğ‘‡ğ‘‡ğ‘‡ğ‘‡ğµğµ, as shown in Figure 8(c). The peak of the contrast curve determines 
the pixel having the highest contrast value, i.e., the best focal position. The parfocal point can also 
be plotted on the contrast curve. In Figure 8(c), the pixel distance Î”N between the parfocal point 
and the peak contrast point on the curve indicates a physical distance along the axial direction. 
This distance represents the distance between the current position of the objective lens and the 
optimal focus position of the objective lens, i.e., one needs to axially move the objective lens by 
this distance for best focusing. While the imaging sensor is centered at the field of view of the 
objective lens, the focusing sensor can be shifted away from the center of the field of view. As 
such, the focusing sensor â€˜seesâ€™ the image data before the imaging sensor â€˜seesâ€™ the same region.  
This article is protected by copyright. All rights reserved.
 
Omnyx 
(now 
Inspirata) 
 VL120 Brightfield 120 slides 40Ã—: 80 sec/slide 
60Ã—: 200 sec/slide Area sensor 
 Independent 
dual sensor 
scanning 
Table 1. A list of commercially-available WSI scanners and automated microscopy systems. Note: every attempt was 
made to include accurate data in this table at the time of writing this article. The autofocusing methods were best 
estimated based on the product instruction manuals and related patents.  
2. Focus map surveying  
Focus map surveying is the most adopted autofocusing method in commercially available WSI 
systems. Manufactories are in favor of using this approach because of two main reasons: 1) it 
requires no additional optical hardware and is robust for different types of samples, and 2) no or 
less intellectual property issue. Here we will first discuss the choice of focus measure in Section 
2.1. We will then discuss focus map generation and focus quality control in Section 2.2.  
2.1. Z-stack acquisition and focus-measure calculation 
The principle of this method is shown in Figure 1, where the camera is used to acquire z-stack 
images of the specimen when the sample or the objective lens is axially scanned to different 
positions. From the resulting z-stack, a certain figure of merit of each image, such as image 
contrast, entropy, spatial frequency content, is extracted for measuring the quality of focus. It is 
also common to acquiring images while calculating the figure of merit, and choosing the image 
corresponding to the peak (or valley) of the figure of merit, or by performing a search to optimize 
the figure of merit. By repeating this searching process for different tiles of the microscope slide, 
the well-focused digital whole slide image can be obtained.  
 
 This article is protected by copyright. All rights reserved.
 
Similarly, a â€˜volume cameraâ€™ consisted of multiple linear CCDs coupled with fibers can be 
arranged with a tilted angle for autofocusing129. Bravo et al reported the use of 9 sensors coupled 
with fibers to acquire images at different focal planes for real-time image-based autofocusing44.   
4.4. Phase detection  
Phase detection autofocusing has been used in most digital single-lens reflex cameras (DLSR)130. 
It is typically achieved by dividing the incoming light into pairs of images. It then measures the 
distance between the two images and infers the defocus amount. The â€˜phaseâ€™ here is referred to 
the translational shift between the two images (or the phase shift in the Fourier domain).  
Inspired by the phase detection concept in photography, we have developed an autofocusing 
add-on kit to perform WSI using a regular microscope131. As shown in Figure 9(a), two pinhole-
modulated cameras are attached to the eyepiece for phase detection autofocusing. By adjusting the 
positions of the pinholes, one can effectively change the view angles through the two eyepiece 
ports. If the sample is placed at the in-focus position, the two captured images will be identical. If 
the sample is placed at an out-of-focus position, the sample will be projected at two different view 
angles, causing a translational shift in the two captured images. The translational shift is 
proportional to the defocus distance of the sample. Therefore, by identifying the translational shift 
of the two captured images via phase correlation, the optimal focal position of the sample can be 
recovered without a z-scan.  
 
 This article is protected by copyright. All rights reserved.
 
 
Figure 1. The traditional axial scanning procedure for autofocusing. For a selected region of interest, a z-stack is 
acquired and used to determine the focus position using a certain figure of merit.  
 
 An important aspect of this approach is to choose a proper figure of merit to measure the quality 
of focus. When the specimen is in focus, the captured image should demonstrate large image 
contrast, a large range of intensity values, and sharp edges. Quantitatively, a good figure of merit 
should be acutely sensitive to focus, monotonically decreasing and symmetric about the peak, and 
contains no prominent local maxima outside of the peak, as shown in Figure 1. Accuracy is clearly 
of utmost importance. In the case of WSI and automated microscopy, minimizing the computation 
time is also critical.  
Several previous studies have evaluated and compared a list of common focus measures22-27. 
Table 2 lists a dozen common focus measures that are intuitive and computationally simple. In 
general, they can be categorized into 4 groups23: (1) derivative-based measures such as Brenner 
gradient, Tenenbaum gradient, energy Laplace, Gaussian derivative, sum of wavelet coefficients, 
ratio of wavelet coefficients, power-weighted average, and power log-log slope, (2) statistical-
based measures such as image contrast, normalized variance, auto-correlation, and standard 
deviation-based correlation, (3) histogram-based measures such as histogram range, histogram 
entropy, and weight histogram sum, and (4) intuitive-based measures such as thresholded content.   
With a chosen focus measure for certain applications, the next step is to estimate the focus 
position using the calculated focus measure from the acquired images. A fitted function can be 
used to find the peak (or valley) from the figure of merit data points, obviating the need to acquire 
This article is protected by copyright. All rights reserved.
 
 
Figure 9. Phase detection for real-time image-based autofocusing. (a) Two pinhole-modulated cameras are attached 
to the eyepiece ports for phase detection autofocusing. If the sample is placed at an out-of-focus position, it will be 
imaged at two different view angles, causing a translational shift in the two captured images through the eyepiece 
ports. Modified from Ref. 131. (b) A dual-pinhole mask is placed at the pupil plane for light modulation. The captured 
image from the focusing sensor contains two copies of the object and the defocus distance can be recovered based on 
the translational shift between the two copies. Modified from Ref. 132. (c) A wedge plate is inserted into the pupil plane 
to direct half of the beam to a slightly tilted angle. As such, the captured image from the focusing sensor contains two 
copies of the sample separated by a certain distance. Similarly, the defocus distance can be recovered from the 
translational shift of the two copies. Modified from Ref. 133. 
 
Figure 9(b) shows another autofocusing configuration using the phase detection concept132. A 
dual-pinhole mask is placed at the pupil plane to modulate the light from the sample. Instead of 
using two pinhole-modulated cameras, only one focusing sensor is used to capture the image 
modulated by the dual-pinhole mask. In this case, the captured image from the focusing sensor 
contains two copies of the sample and the translational shift of these two copies is proportional to 
the defocus distance. Inset of Figure 9(b) shows a raw image captured by the focusing sensor, 
This article is protected by copyright. All rights reserved.
 
images near the focus. The choice of curve fitting model directly affects the number of images 
needed. Typical fitting models include polynomial28, Lorentzian25, and Gaussian models29, 30. A 
polynomial fit may closely approximate the figure of merit data points that are close to the focal 
plane. An nth-order function, however, requires a minimum of n+1 images to be acquired, thus 
drastically increasing image acquisition time when a higher-order fitting curve is employed. It may 
also fail if the focus plane is substantially outside of the depth of field. Yazdanfar et al. have 
demonstrated a Lorentzian function for fitting the Brenner gradient focus measure25. Using this 
empirical model, only 3 images are needed to determine the focal plane. Similarly, Gaussian fitting 
model with 3 unknown parameters has been demonstrated for fluorescence microscopy with an 
electrically tunable lens30. The choice of fitting model is an important topic for each of the chosen 
focus measure and the related microscopy applications. Further research in this direction is highly 
desired.   
 
Focus measure Equation Comments 
Brenner gradient31 ğ¹ğ¹ğµğµğµğµğµğµğµğµğµğµğµğµğµğµ = âˆ‘ âˆ‘ ï¿½ğ¼ğ¼(ğ‘¥ğ‘¥ + 2, ğ‘¦ğ‘¦) âˆ’ ğ¼ğ¼(ğ‘¥ğ‘¥, ğ‘¦ğ‘¦)ï¿½2
ğ‘¦ğ‘¦ğ‘¥ğ‘¥ , where ğ¼ğ¼(ğ‘¥ğ‘¥, ğ‘¦ğ‘¦) is the 
captured 2D intensity image. 
 High autofocusing accuracy for 
different samples23, 25. 
Tenenbaum 
gradient32 
 ğ¹ğ¹ğ‘‡ğ‘‡ğµğµğµğµğµğµğµğµğ‘‡ğ‘‡ğµğµğ‘‡ğ‘‡ğ‘‡ğ‘‡ = âˆ‘ âˆ‘ (ğ‘†ğ‘†ğ‘¥ğ‘¥(ğ‘¥ğ‘¥, ğ‘¦ğ‘¦)2 + ğ‘†ğ‘†ğ‘¦ğ‘¦(ğ‘¥ğ‘¥, ğ‘¦ğ‘¦)2)ğ‘¦ğ‘¦ğ‘¥ğ‘¥ , where ğ‘†ğ‘†ğ‘¥ğ‘¥(ğ‘¥ğ‘¥, ğ‘¦ğ‘¦) and 
ğ‘†ğ‘†ğ‘¦ğ‘¦(ğ‘¥ğ‘¥, ğ‘¦ğ‘¦) are the resultant images by convoluting ğ¼ğ¼(ğ‘¥ğ‘¥, ğ‘¦ğ‘¦) with the 
kernels [-1 0 1; -2 0 2; -1 0 1] and [1 2 1;0 0 0;-1 2 -1], respectively.           
Well performed for subsampled 
images and robust to random 
noises23, 33. 
Energy Laplace34 ğ¹ğ¹ğµğµğµğµğµğµğµğµğ‘‡ğ‘‡ğ‘¦ğ‘¦_ğ¿ğ¿ğ‘‡ğ‘‡ğ¿ğ¿ğ¿ğ¿ğ‘‡ğ‘‡ğ¿ğ¿ğµğµ = âˆ‘ âˆ‘ [ğ¼ğ¼(ğ‘¥ğ‘¥ âˆ’ 1, ğ‘¦ğ‘¦) + ğ¼ğ¼(ğ‘¥ğ‘¥ + 1, ğ‘¦ğ‘¦) + ğ¼ğ¼(ğ‘¥ğ‘¥, ğ‘¦ğ‘¦ âˆ’ 1) +ğ‘¦ğ‘¦ğ‘¥ğ‘¥
ğ¼ğ¼(ğ‘¥ğ‘¥, ğ‘¦ğ‘¦ + 1) + 4ğ¼ğ¼(ğ‘¥ğ‘¥ âˆ’ 1, ğ‘¦ğ‘¦)]2  
 Well performed for tuberculosis 
detection33, 35. 
Gaussian derivative36 
 ğ¹ğ¹ğºğºğ‘‡ğ‘‡ğºğºğºğºğºğºğºğºğ‘‡ğ‘‡ğµğµ = 1
ğ‘‹ğ‘‹âˆ™ğ‘Œğ‘Œ âˆ‘ âˆ‘ [ğ¼ğ¼(ğ‘¥ğ‘¥, ğ‘¦ğ‘¦) âˆ— ğºğºğ‘¥ğ‘¥(ğ‘¥ğ‘¥, ğ‘¦ğ‘¦, ğœğœ)]2 + ï¿½ğ¼ğ¼(ğ‘¥ğ‘¥, ğ‘¦ğ‘¦) âˆ—ğ‘¦ğ‘¦ğ‘¥ğ‘¥
ğºğºğ‘¦ğ‘¦(ğ‘¥ğ‘¥, ğ‘¦ğ‘¦, ğœğœ)ï¿½2, where ğºğºğ‘¥ğ‘¥ and ğºğºğ‘¦ğ‘¦ are the first-order Gaussian 
derivatives in x- and y-direction at scale ğœğœ. 
 Robust against noises with a 
proper selection of parameter ğœğœ36. 
Sum of wavelet 
coefficients37, 38 
 ğ¹ğ¹ğºğºğºğºğ‘ ğ‘ _ğ‘¤ğ‘¤ğ‘‡ğ‘‡ğ‘¤ğ‘¤ğµğµğ¿ğ¿ğµğµğ‘¤ğ‘¤ = âˆ‘ |ğ‘Šğ‘Šğ»ğ»ğ¿ğ¿(ğ‘¥ğ‘¥, ğ‘¦ğ‘¦)| + |ğ‘Šğ‘Šğ¿ğ¿ğ»ğ»(ğ‘¥ğ‘¥, ğ‘¦ğ‘¦)| + |ğ‘Šğ‘Šğ»ğ»ğ»ğ»(ğ‘¥ğ‘¥, ğ‘¦ğ‘¦)|ğœ”ğœ” , where 
ğœ”ğœ” is the corresponding window in the DWT sub-regions. ğ‘Šğ‘Šğ»ğ»ğ»ğ», ğ‘Šğ‘Šğ¿ğ¿ğ»ğ» 
and ğ‘Šğ‘Šğ»ğ»ğ»ğ» are the level-1 two-dimension DWT sub-regions. 
 A common derivative-based focus 
measure37, 38. 
Ratio of wavelet 
coefficients39 
 ğ¹ğ¹ğµğµğ‘‡ğ‘‡ğ‘¤ğ‘¤ğºğºğ‘Ÿğ‘Ÿ_ğ‘¤ğ‘¤ğ‘‡ğ‘‡ğ‘¤ğ‘¤ğµğµğ¿ğ¿ğµğµğ‘¤ğ‘¤ = ğ‘€ğ‘€ğ»ğ»2 ğ‘€ğ‘€ğ¿ğ¿2â„ , ğ‘€ğ‘€ğ»ğ»2 = âˆ‘ âˆ‘ ğ‘Šğ‘Šğ»ğ»ğ¿ğ¿ğµğµ(ğ‘¥ğ‘¥, ğ‘¦ğ‘¦)2 +ğ‘¥ğ‘¥ğ‘¦ğ‘¦ğ¾ğ¾
ğ‘Šğ‘Šğ¿ğ¿ğ»ğ»ğµğµ(ğ‘¥ğ‘¥, ğ‘¦ğ‘¦)2 + ğ‘Šğ‘Šğ»ğ»ğ»ğ»ğµğµ(ğ‘¥ğ‘¥, ğ‘¦ğ‘¦)2 , ğ‘€ğ‘€ğ¿ğ¿2 = âˆ‘ ğ‘Šğ‘Šğ¿ğ¿ğ¿ğ¿ğ¾ğ¾(ğ‘¥ğ‘¥, ğ‘¦ğ‘¦)2ğ‘¥ğ‘¥ğ‘¦ğ‘¦ , where ğ‘Šğ‘Šğ¿ğ¿ğ¿ğ¿ğ¿ğ¿ is 
the Kth level DWT low-frequency sub-region. ğ‘Šğ‘Šğ»ğ»ğ»ğ»ğ»ğ», ğ‘Šğ‘Šğ¿ğ¿ğ»ğ»ğµğµ 
and ğ‘Šğ‘Šğ»ğ»ğ»ğ»ğµğµ are the level-n two-dimension DWT sub-regions. 
 Well performed for common 
microscopic images39. 
Power-weighted 
average40, 41 
 ğ¹ğ¹ğºğºğµğµğ‘‡ğ‘‡ğµğµğ‘¥ğ‘¥(ğ‘§ğ‘§) = âˆ‘ âˆ‘ [ğ‘“ğ‘“(ğ‘¥ğ‘¥, ğ‘¦ğ‘¦) âˆ— ğ¼ğ¼ğ‘§ğ‘§(ğ‘¥ğ‘¥, ğ‘¦ğ‘¦)]2ğ‘¦ğ‘¦ğ‘¥ğ‘¥ ï¿½âˆ‘ âˆ‘ ğ¼ğ¼ğ‘§ğ‘§(ğ‘¥ğ‘¥, ğ‘¦ğ‘¦)ğ‘¦ğ‘¦ğ‘¥ğ‘¥ ï¿½2ï¿½  and 
ğ¹ğ¹ğ¿ğ¿ğ‘Ÿğ‘Ÿğ‘¤ğ‘¤ğµğµğµğµ_ğ‘¤ğ‘¤ğµğµğºğºğ‘‡ğ‘‡â„ğ‘¤ğ‘¤ = âˆ‘ ğ‘§ğ‘§ğ¹ğ¹ğ‘§ğ‘§(ğ‘§ğ‘§)ğ‘ ğ‘ ğ‘§ğ‘§ âˆ‘ ğ¹ğ¹ğ‘§ğ‘§(ğ‘§ğ‘§)ğ‘ ğ‘ ğ‘§ğ‘§â„ , where ğ‘“ğ‘“(ğ‘¥ğ‘¥, ğ‘¦ğ‘¦) is high-pass 
or band-pass filter, âˆ— stands for the convolution operator, ğ¼ğ¼ğ‘§ğ‘§(ğ‘¥ğ‘¥, ğ‘¦ğ‘¦) is 
the grey level intensity of pixel (x, y) at z position. m is an integer 
chosen by the user for different applications. 
 Well performed for phase-contrast 
autofocusing40-44. 
 
This article is protected by copyright. All rights reserved.
 
where two copies of the sample can be seen from this image. The distance between the two copies 
can be recovered via autocorrelation analysis shown in Figure 9(b).  
Figure 9(c) shows a similar phase detection scheme by Silvestri et al.133. Same as the dual-
pinhole modulation approach, only one camera is used for the focusing purpose. A wedge plate is 
inserted into the pupil plane to direct half of the beam to a slightly tilted angle. As such, the 
captured image from the focusing sensor contains two copies of the sample separated by a certain 
distance. The defocus distance can be recovered from the translational shift of the two copies.  
For the configurations shown in Figure 9(a) and 9(b), pinhole masks are used to restrict the 
light at the pupil plane. Therefore, they have relatively long autofocusing ranges. The system in 
Figure 9(c), on the other hand, has a short autofocusing range. Using the dual-pinhole mask does 
not prevent its applications in fluorescence microscopy. One can choose a beam splitter cube to 
direct the strong excitation light through the dual-pinhole mask. Weak fluorescence emissions 
from the sample can be directed to the imaging camera. The configuration in Figure 9(b) has been 
demonstrated for fluorescence WSI132.       
4.5. Dual-LED illumination   
Dual-LED illumination has recently been demonstrated for single-frame autofocusing while the 
sample is in continuous motion134-138. Figure 10(a) shows one of the reported configurations where 
two near-infrared LEDs are placed at the back focal plane of the condenser lens for sample 
illumination134. These two LEDs illuminate the sample from two different incident angles and they 
can be treated as spatially coherent light sources. A hot mirror is used to direct the near-infrared 
light to the focusing sensor shown in Figure 10(a). As such, the captured image from the focusing 
sensor contains two copies of the sample separated by a certain distance. In particular, the focusing 
sensor is placed at a preset offset distance with respect to the imaging sensor. When the sample is 
placed at the in-focus position, the captured image from the focusing sensor contains two copies 
of the sample profile. Similar to the dual-pinhole mask approach, one can recover the defocus 
distance by identifying the separation of the two copies through autocorrelation analysis. The 
This article is protected by copyright. All rights reserved.
 
Power log-log 
slope45 
 ğ¹ğ¹ğ‘ƒğ‘ƒğ¿ğ¿ğ¿ğ¿ğ‘ƒğ‘ƒ is the log-log slope of the one-dimensional power spectral 
density ğ¹ğ¹ğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒ of image I, where  ğ¹ğ¹ğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒ = log(ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘(ğ¹ğ¹ğ¹ğ¹(ğ¼ğ¼))2) and FT 
denotes as Fourier transform.  
 Well performed for focus quality 
control in high-content screening45, 
46.  
Image contrast25 ğ¹ğ¹ğ¿ğ¿ğ‘Ÿğ‘Ÿğµğµğ‘¤ğ‘¤ğµğµğ‘‡ğ‘‡ğºğºğ‘¤ğ‘¤ = (ğ¼ğ¼ğ‘ ğ‘ ğ‘‡ğ‘‡ğ‘¥ğ‘¥ âˆ’ ğ¼ğ¼ğ‘ ğ‘ ğºğºğµğµ) (ğ¼ğ¼ğ‘ ğ‘ ğ‘‡ğ‘‡ğ‘¥ğ‘¥ + ğ¼ğ¼ğ‘ ğ‘ ğºğºğµğµ)â„ , where ğ¼ğ¼ğ‘ ğ‘ ğ‘‡ğ‘‡ğ‘¥ğ‘¥ and ğ¼ğ¼ğ‘ ğ‘ ğºğºğµğµ are 
the maximum and minimum grey level intensity, respectively.  
 A common statistical-based focus 
measure25. 
Normalized 
variance22 
 ğ¹ğ¹ğµğµğ‘Ÿğ‘Ÿğµğµğ‘ ğ‘ ğµğµğ‘‡ğ‘‡_ğ‘¤ğ‘¤ğ‘‡ğ‘‡ğµğµğºğºğ‘‡ğ‘‡ğµğµğ¿ğ¿ğµğµ = 1 (ğ‘‹ğ‘‹ âˆ™ ğ‘Œğ‘Œ âˆ™ ğœ‡ğœ‡)â„ âˆ‘ âˆ‘ (ğ¼ğ¼(ğ‘¥ğ‘¥, ğ‘¦ğ‘¦) âˆ’ ğœ‡ğœ‡)2ğ‘¦ğ‘¦ğ‘¥ğ‘¥ , where ğœ‡ğœ‡ is the 
mean gray level of the image. 
 Well performed for blood smear 
and pap smear autofocusing23, 24, 26. 
Auto-correlation47, 48 ğ¹ğ¹ğ‘‡ğ‘‡ğºğºğ‘¤ğ‘¤ğ‘Ÿğ‘Ÿğ¿ğ¿ğ‘Ÿğ‘Ÿğµğµğµğµ = âˆ‘ âˆ‘ ğ¼ğ¼(ğ‘¥ğ‘¥, ğ‘¦ğ‘¦) âˆ™ ğ¼ğ¼(ğ‘¥ğ‘¥ + 1, ğ‘¦ğ‘¦)ğ‘¦ğ‘¦ğ‘¥ğ‘¥ âˆ’ âˆ‘ âˆ‘ ğ¼ğ¼(ğ‘¥ğ‘¥, ğ‘¦ğ‘¦) âˆ™ ğ¼ğ¼(ğ‘¥ğ‘¥ + 2, ğ‘¦ğ‘¦)ğ‘¦ğ‘¦ğ‘¥ğ‘¥  Well performed for fluorescence 
microscopy23, 49. 
Standard deviation-
based correlation47, 48 ğ¹ğ¹ğ¿ğ¿ğ‘Ÿğ‘Ÿğµğµğµğµ_ğºğºğ‘¤ğ‘¤ğ‘‡ğ‘‡ğ‘‡ğ‘‡ğµğµğ‘¤ğ‘¤ = âˆ‘ âˆ‘ ğ¼ğ¼(ğ‘¥ğ‘¥, ğ‘¦ğ‘¦) âˆ™ ğ¼ğ¼(ğ‘¥ğ‘¥ + 1, ğ‘¦ğ‘¦)ğ‘¦ğ‘¦ğ‘¥ğ‘¥ âˆ’ ğ‘‹ğ‘‹ Â· ğ‘Œğ‘Œ âˆ™ ğœ‡ğœ‡2  Robust to noises23. 
Histogram range50 ğ¹ğ¹ğµğµğ‘‡ğ‘‡ğµğµğ‘‡ğ‘‡ğµğµ = maxğ¼ğ¼ (â„(ğ¼ğ¼) > 0) âˆ’ mğ‘–ğ‘–ğ»ğ»ğ¼ğ¼ (â„(ğ¼ğ¼) > 0) , where â„(ğ¼ğ¼) is image 
histograms (i.e., the number of pixels with intensity ğ¼ğ¼ in an image). 
 Performance depends on samples 
and imaging methods23, 50. 
Histogram entropy50 ğ¹ğ¹ğµğµğµğµğ‘¤ğ‘¤ğµğµğ‘Ÿğ‘Ÿğ¿ğ¿ğ‘¦ğ‘¦ = âˆ’ âˆ‘ ğ‘ğ‘ğ¼ğ¼ Â· log2(ğ‘ğ‘ğ¼ğ¼)ğ¼ğ¼ , where ğ‘ğ‘ğ¼ğ¼ = â„(ğ¼ğ¼) (ğ‘‹ğ‘‹ âˆ™ ğ‘Œğ‘Œ)â„ is the 
probability of a pixel with intensity ğ¼ğ¼. 
 Well performed for sinusoidal and 
binary images50.   
Weight histogram 
sum26, 51 
 ğ¹ğ¹ğ‘Šğ‘Šğ»ğ»ğ‘ƒğ‘ƒ = âˆ‘ ï¿½ï¿½â„(ğ¼ğ¼)5 âˆ™ ğ¼ğ¼(ğ‘¥ğ‘¥, ğ‘¦ğ‘¦)5 âˆ™ 10âˆ’15ï¿½ğ¼ğ¼ , where the fifth root and fifth 
potency are empirical results. 
 Well performed for fluorescence 
bacterial samples26, 51. 
Thresholded 
content22, 52 ğ¹ğ¹ğ‘¤ğ‘¤â„_ğ¿ğ¿ğ‘Ÿğ‘Ÿğµğµğ‘¤ğ‘¤ = âˆ‘ âˆ‘ ğ¼ğ¼(ğ‘¥ğ‘¥, ğ‘¦ğ‘¦)ğ‘¦ğ‘¦ğ‘¥ğ‘¥ , where ğ¼ğ¼(ğ‘¥ğ‘¥, ğ‘¦ğ‘¦) â‰¥ ğœƒğœƒ. ğœƒğœƒ is the threshold Fast computation; a good choice 
for the coarse searching26. 
Table 2. Common figure of merits for measuring the focus quality. 
The focus measures listed in Table 2 are mainly designed for incoherent microscopy with 
intensity-only measurements. Another important property of light wave is phase, which 
characterizes the optical delay accrued during propagation. Light detectors such as image sensors 
and photographic plates can only measure intensity variation of the light waves. Phase information 
is lost during the recording process. Consequently, phase measurement often involves additional 
experimental complexity, typically by requiring light interference with a known field53, 54, or via a 
phase retrieval process where the complex amplitude is recovered from intensity measurements55.  
Coherent microscopy uses both intensity and phase as the focus measure. The autofocusing 
process can be performed after the data has been acquired. As one example, Fourier ptychography 
is a coherent microscopy technique that has been demonstrated for WSI56. Unlike in conventional 
microscopy where resolution and imaging field of view need to be traded off against each other, 
Fourier ptychography can achieve both high resolution and wide field of view via a low-NA 
objective lens and angle-varied illumination. Regular WSI platform stitches the captured intensity 
images in the spatial domain to expand the field of view. Fourier ptychography, on the other hand, 
This article is protected by copyright. All rights reserved.
 
preset offset arrangement in Figure 10(a) is used to improve the accuracy of autocorrelation 
analysis when the defocus distance is small. It can also generate out-of-focus contrast for 
transparent specimens. If the sample motion direction is perpendicular to the direction of the 
translational shift, the autofocusing process can be implemented even with continuous sample 
motion. This dual-LED scheme has also been demonstrated for focus map surveying with only one 
main camera136.   
 
 
Figure 10. Dual-LED illumination for single-frame autofocusing. (a) Two near-infrared LEDs are placed at the back 
focal plane of the condenser lens for illuminating the sample from two different angles. A hot mirror is used to direct 
the near-infrared light to the focusing sensor with a preset offset. The defocus distance is related to the separation of 
the two-copy image captured by the focusing sensor. (b1) Color-multiplexed dual-LED illumination for single-frame 
autofocusing. A red and a green LED are turned on for generating a red and green copy on the color image sensor. 
(b2) OpenWSI system based on the color-multiplexed dual-LED autofocusing scheme. Modified from Ref.138.     
 
 Figure 10(b1) shows a further development of the dual-LED approach using color multiplexed 
illumination137, 138. In this scheme, a color LED array is used for sample illumination. For regular 
brightfield image acquisition, all LED elements are turned on as shown in the left part of Figure 
10(b1). In between two brightfield acquisitions, a red and a green LED are turned on for color-
This article is protected by copyright. All rights reserved.
 
stitches the information in the Fourier domain to expand the spatial frequency bandwidth. 
Autofocusing of Fourier ptychography is performed in the ptychographic phase retrieval process57, 
58, where the defocus pupil aberration can be jointly recovered with the complex object59-61. At the 
end of the reconstruction, the synthesized information in the Fourier domain generates a high-
resolution, complex-valued object image that retains the original large field of view set by the low-
NA objective lens. Similar coherent imaging procedures can also be performed at the detection 
path via aperture or diffuser modulation62-67. In this case, the recovered complex wavefront can be 
digitally propagated to any plane along the optical axis after reconstruction. A focus measure with 
both intensity and phase can be used to determine the best focal plane of the object68-78. A detailed 
discussion on coherent microscopy and the related focus measures are beyond the scope of this 
review article. In the following, we focus our discussions on regular incoherent microscopy.  
 
2.2. Focus map, skipping tiles, and focus quality control     
By repeating the z-stack autofocusing process for every tile, it is straightforward to generate a 
high-resolution, well-focused whole slide image of the specimen. However, as indicated above, 
the autofocusing process can take a significant amount of time to acquire z-stacks at multiple 
positions. Assuming a rate of 20 frames per second to acquire images, surveying focus at 5 
different focal positions would take 0.25 seconds per tile. As a result, an image with 500 tiles can 
take as much as 150 seconds to acquire, not including the deceleration, acceleration, settling time 
for moving the slide to different lateral and axial positions. Therefore, it is not a feasible solution 
to perform autofocusing on every tile using the traditional image-based focus measure approach. 
To address the time burden, many WSI systems create a focus map prior to scanning, or survey 
focus points every n tiles or lines, in effect skipping areas to save time6. The number and the 
locations of the focus points are often made user selectable.  
Figure 2(a) shows the procedures of the focus map surveying approach. The system will first 
select focus points based on the sampleâ€™s feature and distribute them evenly over the entire slide. 
Each focus point is triangulated to create a focus map of the tissue surface, in effect filling in the 
This article is protected by copyright. All rights reserved.
 
multiplexed illumination. If the sample is placed at an out-of-focus position, the red and the green 
copy will be separated by a certain distance, as shown in the insets of Figure 10(b1). One can then 
identify the translational shift of the red- and green-channel images by maximizing the image 
mutual information or cross-correlation139, 140. The resulting translational shift is used for dynamic 
focus correction in the scanning process.  
Figure 10(b2) shows an open-source WSI platform, termed OpenWSI, based on the color-
multiplexed dual-LED autofocusing scheme138. This OpenWSI platform is built with low-cost, 
off-the-shelf components including a programmable LED array, a photographic lens, and a 
computer numerical control (CNC) router. Coarse axial adjustment is performed using the CNC 
router and the precise adjustment is performed using the ultrasonic motor ring within the 
photographic lens. The system has a resolution of ~0.7 Âµm using a 20X objective lens. It can 
acquire a whole slide image of 225 mm2 region in ~2 mins. Since a programable LED array is used 
for sample illumination in this system, it can also be used for quantitative phasing imaging via 
Fourier ptychography.     
4.6. Deep learning approaches   
Deep learning has been demonstrated as a powerful tool for solving inverse problems. With the 
advent of accelerated computing and deep learning frameworks such as TensorFlow and PyTorch, 
researchers have also explored various deep learning-based solutions for autofocusing21, 92, 139-150. 
As shown in Figure 11, the reported deep-learning solutions can be, in general, categorized into 
two groups.  
 
 This article is protected by copyright. All rights reserved.
 
blanks. Delaunay triangulation is a typical method for generating the focus map6. As shown in 
Figure 2(b), line scanners typically achieve better autofocusing performance than traditional 2D 
area sensors because linear sensors can change focus at a shorter interval.  
Regular 1D and 2D image sensors need to have high illumination intensity to quickly register 
light levels before the sample motion causes smearing of the image. Time delay integration (TDI) 
sensor overcomes this illumination limitation by having multiple rows of elements that each shift 
their partial measurements to the adjacent row synchronously with the motion of the image across 
the array of elements44. TDI sensors are often the choice of low-light applications such as 
fluorescence microscopy with low photon budgets. The disadvantage of TDI sensor is the 
requirement of precisely synchronized sample scanning for generating an image. Rescan of the 
sample is needed for imaging multiple depths or fluorescence channels. Precise co-localization of 
different depths or different fluorescence colors can be a challenge for post-acquisition processing. 
The use of TDI sensors also lacks the imaging flexibility for research microscopy in general.    
An alternative approach to generating the focus map is to perform autofocusing in every n tiles, 
termed â€˜skipping tilesâ€™ in Figure 2(b). In this case, it assumes the focused tile shares the same focus 
position with its adjacent tiles. The focusing performance is, however, worse than the focus map 
approach as it may contain more out-of-focus regions as shown in Figure 2(b). The skipping tiles 
approach, on the other hand, does not need to travel back to a certain axial position with sub-
micron accuracy. The requirement of motion repeatability is not as stringent as that in the focus 
map approach. Nevertheless, more focus points can increase the accuracy of the overall focusing 
performance for both approaches, at the expense of additional time for autofocusing.    
 
 This article is protected by copyright. All rights reserved.
 
 
Figure 11. Deep learning approaches for autofocusing. (a) A neural network is trained to output the defocus distance 
from an input defocused image. (b) A neural network is trained to output an in-focus image based on the input 
defocused image.  
 
 The first group is to predict the defocus distance or to locate the out-of-focus regions based on 
one or more input defocused images21, 92-94, 140, 141, 144, 147, 149, 150. For example, Jiang et al. employed 
a convolutional neural network (CNN) to estimate the defocus distance based on the transform- 
and multi-domain inputs141. By adding the Fourier spectrum and the autocorrelation of the spatial 
image as the input, the performance and the robustness can be improved compared to that only 
with the spatial image as the input. Dastidar et al. further improved the performance by using the 
difference of two defocused images as the input of the CNN140. Shajkofci et al. reported the use of 
a CNN-based sharpness function as the focus measure for three-shot autofocusing147. Pinkard et 
al. designed a fully connected Fourier neural network with the additional off-axis LEDs as the 
illumination source to predict the defocus distance144. Yang et al.94 and Kohlberger et al.21 have 
developed networks to quantify and localize the out-of-focus regions in WSI. The severity of the 
out-of-focus regions is treated as a classification problem with 30 classes21. 
The second group of developments is to output an in-focus image based on an input defocused 
image142, 143, 146, 148. The network is, essentially, to perform blind deconvolution. Typically network 
architectures include U-net151 and conditional generative adversarial network (cGAN)152. For 
This article is protected by copyright. All rights reserved.
 
      
Figure 2. (a) Focus map generation procedures. The green bars represent the calculated figure of merits at different 
focus points. The red bars represent the interpolated focus points. (b) Comparison between the focus map approach 
and the skipping tiles approach. The green crosshairs represent the focus points used to calculate the focus map. The 
blue dashed lines represent the focus positions interpolated between the selected focus points. Red boxes represent the 
focal plane for each field of view using a 2D image sensor or a 1D linear sensor. Each red box can be adjusted in the 
z-axis during a scan. Modified from Ref. 6. 
 
After the high-resolution specimen images are acquired, it is often necessary to review the 
images for focus quality control and determine whether certain regions need to be re-scanned. 
Similarly, in high-content screening for drug discovery and genome analysis, it is important to 
identify out-of-focus images for obtaining a clean, unbiased image dataset. Complicating this task 
is the fact that one only has a single-z-depth image instead of a z-stack for analysis. An absolute 
measure of image focus on a single image in isolation, without other user-specified parameters, is 
needed in this case. In the past years, various approaches have been demonstrated for no-reference 
focus quality assessment, including gradient map79-81, contrast map82-84, phase coherency85, 86, 
cumulative probability of blur detection87, 88, visual systemâ€™s equalization of spatial frequency89, 
among others. Jimenez et al. have tested several quality assessment metrics on a database of 
This article is protected by copyright. All rights reserved.
 
example, Wu et al. have employed a cGAN to virtually refocus a two-dimensional fluorescence 
image onto user-defined three-dimensional (3D) surfaces by appending a pre-defined digital 
propagation matrix148. It has also been shown that a blurry microscopy image acquired at an 
arbitrary out-of-focus plane can be virtually refocused to the in-focus position143.       
5. Summary and discussion  
High-content images are desired in many fields of biomedical research as well as in clinical 
applications. Accurate and high-speed autofocusing remains a challenge for WSI and automated 
microscopy. This work has reviewed and discussed various autofocusing techniques from existing 
patents and journal papers. The technical concepts, merits, and limitations of these methods are 
explained and discussed. We summarize the advantages and disadvantages of these techniques in 
Table 3. Among these techniques, the focus map approach is the most adopted technique in 
existing WSI systems due to its simplicity and the absence of intellectual property issues. The 
tilted sensor approach is another very successful technique employed in current Leica and Philips 
WSI systems. The recent dual-LED approach provides a cost-effective solution to develop WSI 
systems that can be made broadly available and utilizable without loss of capacity. The deep 
learning approach, on the other hand, is an emerging direction for tackling autofocusing problems 
without hardware modification. Further work is desired for improving its robustness and the 
generalization capability of handling different types of specimens.  
Some of the autofocusing techniques discussed here can also be employed in the augmented 
reality microscope system. For example, a secondary tilted sensor can be used for locating the 
optimal focus position in real-time. A motorized stage can be used to drive the main camera for 
capturing the in-focus, high-resolution sample images. 
 
Autofocusing 
approach  Advantages Disadvantages 
Focus map 
 â–ª No or less intellectual property issue 
â–ª Require no additional optical hardware  
â–ª Can be used for different imaging modalities  
â–ª Robust and widely adopted for WSI  
 â–ª Require a z-stack for each focus point 
â–ª Mechanical repeatability is critical for sample 
positioning 
â–ª Challenging to handle transparent specimens 
This article is protected by copyright. All rights reserved.
 
pathology slides and reported that the cumulative probability of blur detection is most effective 
among the 6 tested metrics86. Another emerging direction for focus quality control is to convert 
the image assessment process into a classification task using a neural network21, 90-94. For example, 
Senaras et al. reported a â€˜DeepFocusâ€™ network to identify out-of-focus regions in histopathological 
images92. Discussions of deep-learning approaches will be given in Section 4.6.     
3. Reflective-based autofocusing  
Reflective-based autofocusing aims to detect the axial location of a reference plane, which is 
usually the interface between glass and liquid where the cells residue or the air-glass interface at 
the bottom of the cell culture vessels. During experiments, the focus drift correction system will 
repetitively find the axial location of the reference plane and maintain a constant distance between 
the objective lens and the reference plane through a motorized axial driver. In Section 3.1, we will 
discuss a confocal pinhole approach to locate the interfaces. In Section 3.2, we will discuss how 
to use the reflective light displacement to locate the reference plane in real-time. In Section 3.3, 
we will discuss a low-coherence interferometry approach to locate the sample switched by two 
interfaces in real-time.     
3.1. Confocal pinhole detection 
Liron et al. reported a laser reflective autofocusing approach using confocal pinhole detection in 
200695. The optical setup is shown in Figure 3, where a laser beam is expanded and focused onto 
the substrate of the sample (highlighted in red). The reflective light from the substrate passes 
through a confocal pinhole and reaches the photodetector (highlighted in yellow). The fraction of 
laser intensity reflected at an interface is roughly proportional to the square of the refractive index 
difference. For biological specimens located in water (or aqueous buffers) above a glass / plastic 
plate, the reflection from the glass-air interface is about 4% of the incident beam and the reflection 
from the glass-water interface is only 0.4%. The inset of Figure 3 shows a measured intensity curve 
by axially scanning the objective lens to different positions. The first strong peak corresponds to 
This article is protected by copyright. All rights reserved.
 
Confocal pinhole â–ª High accuracy for locating the air-glass interface  
 
 â–ª Require additional confocal optics 
â–ª Time-consuming for z-scan   
â–ª Reflection from other interfaces can be 
overwhelmed by the strong signal from the air-glass 
interface  
Triangulation with 
oblique illumination 
 â–ª High accuracy for locating the air-glass surface of 
a standard coverslip  
â–ª Real-time autofocusing  
 â–ª Require additional illumination and detection 
optics 
â–ª Only work for living cells housed in imaging 
chambers with a standard coverslip. Cannot work 
for microscope slides or thick plastic dish.  
Low-coherence 
interferometry 
 â–ª Can handle transparent specimens 
â–ª Real-time autofocusing 
 â–ª Expensive and complicated Fourier-domain OCT 
setup 
â–ª Precise optical alignment needed  
Independent dual 
sensor scanning 
 â–ª Real-time image-based autofocusing during 
continuous sample motion 
â–ª Effectively avoid the â€˜dead timeâ€™ of camera 
readout   
 â–ª Require a secondary area camera and pulsed 
illumination 
â–ª Require the acquisition of three images for 
autofocusing with a small overlapping portion  
â–ª Relatively short autofocusing range 
Beam splitter array â–ª Real-time image-based autofocusing â–ª Require a secondary area camera 
â–ª Relatively short autofocusing range 
Tilted sensor 
 â–ª Real-time image-based autofocusing  
â–ª Fully compatible with linear and TDI image sensor  
â–ª Fast calculation via contrast curve  
â–ª One of the most successful techniques deployed in 
commercially available WSI systems 
 â–ª Require a secondary focusing sensor 
â–ª A transparent sample may lead to a wrong 
autofocusing calculation since out-of-focus regions 
have a higher contrast 
Phase detection 
 â–ª Real-time image-based autofocusing 
â–ª Can handle transparent specimens via a preset 
offset of the focusing sensor  
 â–ª Require additional camera(s) and relay optics for 
the pinhole mask 
â–ª Precise alignment needed for the pinhole mask 
â–ª Low-pass filtering of the pinhole mask may affect 
the accuracy of the correlation analysis  
Dual-LED 
illumination 
 â–ª Real-time image-based autofocusing 
â–ª Can be implemented with continuous sample 
motion 
â–ª Can handle transparent specimens  
â–ª Relatively long autofocusing range due to the use 
of partially coherent dual-LED illumination 
â–ª Cost effective and compatible with most 
automated microscope platforms 
 â–ª Only work for regular 2D thin slides 
 
Deep learning â–ª Allow single-shot autofocusing 
â–ª Require no additional optical hardware 
 â–ª Relatively short virtual refocusing range 
â–ª Change of optical hardware may affect the 
autofocusing performance  
â–ª The system may fail for new features or new types 
of specimens that have not been trained before 
Table 3. Summary and comparison of different autofocusing techniques. 
 
In the medical realm, one strategy taken by the National Cancer Moonshot initiative to fight 
cancer cooperatively is to create an image database for different cases and connect scientists and 
This article is protected by copyright. All rights reserved.
 
the air-glass interface and the second weaker peak corresponds to the sample-glass interface. Solid 
and dashed lines are results for 100-Âµm and 200-Âµm pinhole. Increasing the confocal pinhole size 
can broaden the width of the peaks as indicated by the dashed line in Figure 3. This adjustment 
can reduce some unwanted interference speckles and facilitate the data analysis process. A two-
stage operation was employed to perform the autofocusing process. The first stage, termed â€˜long 
peak detection searchâ€™, is to locate the strong peak via high-speed axial scanning of the objective 
lens. With the location of the first strong peak, the position of the second peak can be estimated 
by adding the thickness of the glass substrate. The second stage, termed â€˜local peak searchâ€™, 
performs precise peak search over a relatively short range.   
 
 
Figure 3. An autofocusing system using confocal pinhole detection. Laser light is expanded and focused on the 
substrate of the sample. The reflective light, highlighted in yellow, is passed through a confocal pinhole and detected 
by the photodetector. Inset in the top right shows the measured intensity signals by axially scanning the objective lens 
to different positions. The first strong peak corresponds to the air-glass interface and the second weaker peak 
corresponds to the sample-glass interface. Solid and dashed lines are results for 100 Âµm and 200 Âµm pinhole. Modified 
from Ref. 95. 
 
While this confocal detection approach can perform precise autofocusing, its main drawback 
is the requirement of axial scanning to get the trace curve shown in Figure 3. Another drawback is 
This article is protected by copyright. All rights reserved.
 
pathologists for online collaboration. Coupling an automated microscope system with a proper 
autofocusing technique has the potential to convert various biological specimens into high-content 
images and address the challenge of high-throughput microscopy.        
Acknowledgments 
Z. B. and C. G. contributed equally to this work. P. S. acknowledges the support of the Thermo 
Fisher Scientific Fellowship. K. H. acknowledges the support of NSF 1809047. G. Z. 
acknowledges the support of NSF 1700941 and NSF 2012140.  
 
Data Availability Statement 
Research Data are not shared. 
 
References 
[1] F. Ghaznavi, A. Evans, A. Madabhushi, M. Feldman, Annu. Rev. Pathol. 2013, 8, 331-359. 
[2] C. Higgins, Biotech. Histochem. 2015, 90, 341-347. 
[3] M. N. Gurcan, L. E. Boucheron, A. Can, A. Madabhushi, N. M. Rajpoot, B. Yener, IEEE Rev. 
Biomed. Eng. 2009, 2, 147-171. 
[4] M. B. Amin, F. L. Greene, S. B. Edge, C. C. Compton, J. E. Gershenwald, R. K. Brookland, 
L. Meyer, D. M. Gress, D. R. Byrd, D. P. Winchester, CA Cancer J. Clin. 2017, 67, 93-99. 
[5] R. Ferreira, B. Moon, J. Humphries, A. Sussman, J. Saltz, R. Miller, A. Demarzo, Proc AMIA 
Annu Fall Symp, American Medical Informatics Association, 1997, 449 
[6] M. C. Montalto, R. R. McKay, R. J. Filkins, J. Pathol. Inform. 2011, 2. 
[7] A. J. Evans, T. W. Bauer, M. M. Bui, T. C. Cornish, H. Duncan, E. F. Glassy, J. Hipp, R. S. 
McGee, D. Murphy, C. Myers, Arch. Pathol. Lab. Med. 2018, 142, 1383-1387. 
[8] E. Abels, L. Pantanowitz, J. Pathol. Inform. 2017, 8. 
[9] M. K. K. Niazi, A. V. Parwani, M. N. Gurcan, Lancet Oncol. 2019, 20, e253-e261. 
[10] H. R. Tizhoosh, L. Pantanowitz, J. Pathol. Inform. 2018, 9. 
[11] N. Radakovich, M. Nagy, A. Nazha, Networks 2020, 2, 6. 
[12] N. Dimitriou, O. ArandjeloviÄ‡, P. D. Caie, Front. Med. 2019, 6. 
[13] A. Janowczyk, A. Madabhushi, J. Pathol. Inform. 2016, 7. 
[14] Y. Liu, K. Gadepalli, M. Norouzi, G. E. Dahl, T. Kohlberger, A. Boyko, S. Venugopalan, A. 
Timofeev, P. Q. Nelson, G. S. Corrado, arXiv preprint arXiv:1703.02442 2017. 
[15] P.-H. C. Chen, K. Gadepalli, R. MacDonald, Y. Liu, S. Kadowaki, K. Nagpal, T. Kohlberger, 
J. Dean, G. S. Corrado, J. D. Hipp, Nat. Med. 2019, 25, 1453-1457. 
This article is protected by copyright. All rights reserved.
 
the orders of magnitude difference in strength for the two peaks. The weaker peak can easily be 
overwhelmed by the first strong peak, especially for lower magnification objective lenses. In 
Section 3.2, we will discuss a strategy to address the first drawback, i.e., to locate the first peak 
position without performing axial scanning. In Section 3.3, we will discuss another strategy to 
address both drawbacks, i.e., to reduce the signal strength from the first peak and to locate both 
peaks without axial scanning.       
3.2. Triangulation with oblique illumination  
To locate the axial position of an interface without axial scanning, one can illuminate the sample 
with a tilted incident angle and measure the lateral displacement of the reflected beam. The 
triangulation concept for microscopy autofocusing can be dated back to the patent by Reinheimer 
in 197396. In this patent, Reinheimer proposed to restrict a shaped illumination beam to occupy 
only half of the pupil aperture cross-section. As such, the beam reflected from a surface will have 
different lateral displacements when the sample surface is placed at different axial positions. The 
reflected light from the sample surface is detected by two photoelectric transducers for differential 
measurement. The differential signal detected by these two transducers is used to drive the focus 
knob. For example, if the sample surface is placed at the in-focus position, the reflected light will 
be directed to the boundary of the two transducers. The resulting differential signal is 0 and no 
adjustment is needed. If the sample surface is positioned above the in-focus plane, the reflected 
light will shift to one of the transducers. The differential signal is then used to drive down the 
sample stage. Similarly, if the sample surface is positioned below the in-focus plane, the 
differential signal from the two transducers drives up the sample stage. There are some further 
refinements and developments of this original patent in the 1980s and 1990s97-104. These 
developments are, in general, about how to better detect the beam size and the positional shift to 
infer the defocus distance. Similar schemes have also been reported in more recent literatures105-
110. 
 This article is protected by copyright. All rights reserved.
 
[16] J. D. Ianni, R. E. Soans, S. Sankarapandian, R. V. Chamarthi, D. Ayyagari, T. G. Olsen, M. 
J. Bonham, C. C. Stavish, K. Motaparthi, C. J. Cockerell, Sci. Rep. 2020, 10, 1-12. 
[17] J. D. Ianni, R. E. Soans, S. Sankarapandian, R. V. Chamarthi, D. Ayyagari, T. G. Olsen, M. 
J. Bonham, C. C. Stavish, K. Motaparthi, C. J. Cockerell, arXiv preprint arXiv:1909.11212 2019. 
[18] B. E. Bejnordi, M. Veta, P. J. Van Diest, B. Van Ginneken, N. Karssemeijer, G. Litjens, J. A. 
Van Der Laak, M. Hermsen, Q. F. Manson, M. Balkenhol, JAMA 2017, 318, 2199-2210. 
[19] J. R. Gilbertson, J. Ho, L. Anthony, D. M. Jukic, Y. Yagi, A. V. Parwani, BMC Clin. Pathol. 
2006, 6, 4. 
[20] C. Massone, H. P. Soyer, G. P. Lozzi, A. Di Stefani, B. Leinweber, G. Gabler, M. Asgari, R. 
Boldrini, L. Bugatti, V. Canzonieri, Hum. Pathol. 2007, 38, 546-554. 
[21] T. Kohlberger, Y. Liu, M. Moran, P.-H. C. Chen, T. Brown, J. D. Hipp, C. H. Mermel, M. C. 
Stumpe, J. Pathol. Inform. 2019, 10, 39. 
[22] F. C. Groen, I. T. Young, G. Ligthart, Cytometry A 1985, 6, 81-91. 
[23] Y. Sun, S. Duthaler, B. J. Nelson, Microsc. Res. Tech. 2004, 65, 139-149. 
[24] X. Liu, W. Wang, Y. Sun, J. Microsc. 2007, 227, 15-23. 
[25] S. Yazdanfar, K. B. Kenny, K. Tasimi, A. D. Corwin, E. L. Dixon, R. J. Filkins, Opt. Express 
2008, 16, 8670-8677. 
[26] R. Redondo, G. CristÃ³bal, G. B. Garcia, O. Deniz, J. Salido, M. del Milagro Fernandez, J. 
Vidal, J. C. Valdiviezo, R. Nava, B. Escalante-RamÃ­rez, J. Biomed. Opt. 2012, 17, 036008. 
[27] S. Pertuz, D. Puig, M. A. Garcia, Pattern Recognit 2013, 46, 1415-1432. 
[28] W. BÃ¶cker, W. Rolf, W. MÃ¼ller, C. Streffer, Phys. Med. Biol. 1997, 42, 1981. 
[29] S. K. Nayar, Y. Nakagawa, IEEE Trans. Pattern Anal. Mach. Intell. 1994, 16, 824-831. 
[30] Z. Wang, M. Lei, B. Yao, Y. Cai, Y. Liang, Y. Yang, X. Yang, H. Li, D. Xiong, Biomed. Opt. 
Express 2015, 6, 4353-4364. 
[31] J. F. Brenner, B. S. Dew, J. B. Horton, T. King, P. W. Neurath, W. D. Selles, J. Histochem. 
Cytochem. 1976, 24, 100-111. 
[32] T. Yeo, S. Ong, R. Sinniah, Image. Vis. Comput 1993, 11, 629-639. 
[33] O. Osibote, R. Dendere, S. Krishnan, T. Douglas, J. Microsc. 2010, 240, 155-163. 
[34] M. Subbarao, T.-S. Choi, A. Nikzad, Opt. Eng. 1993, 32, 2824-2837. 
[35] M. J. Russell, T. S. Douglas, 2007 29th Annual International Conference of the IEEE 
Engineering in Medicine and Biology Society, IEEE, 2007, 3489-3492 
[36] J. M. Geusebroek, F. Cornelissen, A. W. Smeulders, H. Geerts, Cytometry A 2000, 39, 1-9. 
[37] G. Yang, B. J. Nelson, Proceedings 2003 IEEE/RSJ International Conference on Intelligent 
Robots and Systems IEEE, 2003, 2143-2148 
[38] G. Yang, B. J. Nelson, 2003 IEEE International Conference on Robotics and Automation, 
IEEE, 2003, 3200-3206 
[39] H. Xie, W. Rong, L. Sun, Microsc. Res. Tech. 2007, 70, 987-995. 
[40] M. Bravo-Zanoguera, B. v. Massenbach, A. L. Kellner, J. H. Price, Rev. Sci. Instrum. 1998, 
69, 3966-3977. 
[41] J. H. Price, D. A. Gough, Cytometry A 1994, 16, 283-297. 
This article is protected by copyright. All rights reserved.
 
Figure 4 shows the adoption of the triangulation idea in a modern microscope system, marketed 
as Nikon Perfect Focus System (PFS)111. This system maintains focus by detecting and tracking 
the position of the coverslip surface in real-time. It employs a near-infrared 870-nm LED as the 
light source and a linear CCD sensor as the detector (other detectors such as four-quant photodiode 
and area sensor can also be used here). Predefined by the user is an offset between the reference 
plane and the axial location of the desired focused image. Different from the original patent by 
Reinheimer, the PFS system introduces two offset adjustment lenses in Figure 4 to maintain the 
focus at the desired positional offset from the coverslip surface. When the user changes the offset 
distance, the distance of the two offset adjustment lenses changes, resulting in a shift of the line 
position detected by linear CCD (inset of Figure 4). The positional shift generates a signal to move 
the objective lens along the axial direction until the line position is centered at the linear CCD 
again.  
    
 This article is protected by copyright. All rights reserved.
 
[42] M. A. Oliva, M. Bravo-Zanoguera, J. H. Price, Appl. Opt. 1999, 38, 638-646. 
[43] F. Shen, L. Hodgson, K. Hahn in Digital autofocus methods for automated microscopy, Vol. 
414, Elsevier, 2006, pp.620-632. 
[44] M. E. Bravo-Zanoguera, C. A. Laris, L. K. Nguyen, M. Oliva, J. H. Price, J. Biomed. Opt. 
2007, 12, 034011. 
[45] D. J. Field, N. Brady, Vision Res. 1997, 37, 3367-3383. 
[46] M.-A. Bray, A. N. Fraser, T. P. Hasaka, A. E. Carpenter, J. Biomol. Screen. 2012, 17, 266-
274. 
[47] D. Vollath, J. Microsc. 1987, 147, 279-288. 
[48] D. Vollath, J. Microsc. 1988, 151, 133-146. 
[49] A. Santos, C. Ortiz de SolÃ³rzano, J. J. Vaquero, J. M. Pena, N. Malpica, F. del Pozo, J. 
Microsc. 1997, 188, 264-272. 
[50] L. Firestone, K. Cook, K. Culp, N. Talsania, K. Preston Jr, Cytometry A 1991, 12, 195-206. 
[51] M. Zeder, J. Pernthaler, Cytometry A 2009, 75, 781-788. 
[52] M. L. Mendelsohn, B. H. Mayall, Comput. Biol. Med. 1972, 2, 137-150. 
[53] U. Schnars, C. Falldorf, J. Watson, W. JÃ¼ptner in Digital holography, Vol., Springer, 2015, 
pp.39-68. 
[54] B. Kemper, G. Von Bally, Appl. Opt. 2008, 47, A52-A61. 
[55] J. R. Fienup, Appl. Opt. 1982, 21, 2758-2769. 
[56] G. Zheng, R. Horstmeyer, C. Yang, Nat. Photon. 2013, 7, 739. 
[57] J. M. Rodenburg, Adv. Imaging Electron Phys. 2008, 150, 87-184. 
[58] J. Rodenburg, A. Maiden in Ptychography, Vol., Springer, 2019, pp.2-2. 
[59] X. Ou, G. Zheng, C. Yang, Opt. Express 2014, 22, 4960-4972. 
[60] P. Song, S. Jiang, H. Zhang, X. Huang, Y. Zhang, G. Zheng, APL Photonics 2019, 4, 050802. 
[61] A. J. Williams, J. Chung, X. Ou, G. Zheng, S. Rawal, Z. Ao, R. Datar, C. Yang, R. J. Cote, J. 
Biomed. Opt. 2014, 19, 066007. 
[62] S. Dong, R. Horstmeyer, R. Shiradkar, K. Guo, X. Ou, Z. Bian, H. Xin, G. Zheng, Opt. 
Express 2014, 22, 13586-13599. 
[63] Z. Bian, S. Jiang, P. Song, H. Zhang, P. Hoveida, K. Hoshino, G. Zheng, J. Phys. D: Appl. 
Phys. 2019, 53, 014005. 
[64] P. Song, S. Jiang, H. Zhang, Z. Bian, C. Guo, K. Hoshino, G. Zheng, Opt. Lett. 2019, 44, 
3645-3648. 
[65] S. Jiang, J. Zhu, P. Song, C. Guo, Z. Bian, R. Wang, Y. Huang, S. Wang, H. Zhang, G. 
Zheng, Lab Chip 2020, 20, 1058-1065. 
[66] X. He, Z. Jiang, Y. Kong, S. Wang, C. Liu, Opt. Commun. 2020, 459, 125057. 
[67] C. Shen, A. C. S. Chan, J. Chung, D. E. Williams, A. Hajimiri, C. Yang, Opt. Express 2019, 
27, 24923-24937. 
[68] J. Xu, Y. Kong, Z. Jiang, S. Gao, L. Xue, F. Liu, C. Liu, S. Wang, Appl. Opt. 2019, 58, 3003-
3012. 
[69] P. Langehanenberg, G. von Bally, B. Kemper, 3D Research 2011, 2, 4. 
This article is protected by copyright. All rights reserved.
 
 
Figure 4. The Nikon Perfect Focus System. Light from an infrared LED is shaped by a line aperture and a half-moon 
mask for illuminating the sample substrate at a tilted angle. The reflected light is detected by a linear CCD. Inset 
shows the detected line traces when the sample substrate is scanned to different defocused positions. Two offset 
adjustment lenses are used to maintain the focus at the desired positional offset from the coverslip surface. Modified 
from Ref. 111.    
 
The PFS system is mainly designed to image living cells housed in imaging chambers equipped 
with a standard coverslip. For fixed specimens like tissue slides that are mounted in a high 
refractive index medium (which closely matches that of the coverslip), the refraction index 
difference may not generate sufficient signal to detect the interface surface. Likewise, tissue slides 
with strong absorption profiles often scatter a considerable amount of light, leading to excessive 
This article is protected by copyright. All rights reserved.